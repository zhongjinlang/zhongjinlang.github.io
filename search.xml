<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[并发基础（一）]]></title>
    <url>%2F2019%2F04%2F21%2F%E5%B9%B6%E5%8F%91%E5%9F%BA%E7%A1%80(%E4%B8%80)%2F</url>
    <content type="text"><![CDATA[基本概念并发：同时拥有两个或多个线程，如果程序在单核处理器上运行，多个线程会交替执行，这些线程是同时存在的。如果运行在多核处理器上，此时，程序中的每个线程都将分配到一个处理器核上，因此可同时运行 高并发(high concurrent)：是互联网分布式系统架构设计中必须考虑的因素之一，它通常是指，通过设计保证系统能够同时并行处理很多请求 并发：多个线程操作相同的资源，保证线程安全，合理使用资源 高并发：服务能同时处理很多请求，提高程序性能 CPU多级缓存随着现代半导体工艺的发展，CPU的频率越来远快，相对内存快了一个数量级，对于访存的操作CPU就需要等待主存，这样会导致资源的白白浪费。所以cache的出现是为了解决CPU与内存速度不匹配的问题 cache 的工作原理是基于“局部性”原理，它包含以下两个方面： 时间局部性：如果某个数据被访问，那么不久将来它很可能再次被访问 空间局部性：如果某个数据被访问，那么与它相邻的数据也可能被访问 cache中保存着cpu刚用过的数据或者是循环使用的数据，这时，从cache中读取数据就会很快，减少了cpu等待的时间，提高了系统的性能 缓存带来的问题cache 给系统带来性能上飞跃的同时，也引入了新的问题“缓存一致性问题”。设想如下场景（cpu一共有两个核，core1和core2）：以i++为例，i的初始值是0.那么在开始每个核都存储了i的值0，当第core1块做i++的时候，其缓存中的值变成了1，即使马上回写到主内存，那么在回写之后core2缓存中的i值依然是0，其执行i++，回写到内存就会覆盖第一块内核的操作，使得最终的结果是1，而不是预期中的2 缓存一致性(MESI)为了达到数据访问的一致，需要各个处理器在访问缓存时遵循一些协议，在读写时根据协议来操作，常见的协议有MSI，MESI，MOSI等。我们介绍其中最经典的MESI协议 在MESI协议中，每个cache line有4个状态，可用2个bit表示，它们分别是： M(Modified): 这行数据有效，数据被修改了，和内存中的数据不一致，数据只存在于本Cache中 E(Exclusive): 这行数据有效，数据和内存中的数据一致，数据只存在于本Cache中 S(Shared): 这行数据有效，数据和内存中的数据一致，数据存在于很多Cache中 I(Invalid): 这行数据无效 CPU多级缓存-乱序执行优化处理器为提高运算速度而做出违背代码原有顺序的优化 乱序执行优化出现问题的原因在单核时代处理器做出的优化可以保证执行结果不会远离预期目标，但是，在多核时代却并非如此。在多核时代，同时会有多个核同时执行指令，每一个核的指令都可能被乱序。另外,处理器还引入了L1，L2,…,Ln等多级缓存机制，每个核心都有自己的缓存机制，这样就导致了逻辑次序上后写入内存的数据未必真的最后写入。最后就带来一个问题，如果不做任何防护措施，处理器最终得出的结果和逻辑得出结果会大不相同。比如，在一个核上执行写入操作，并在最后写一个标记用来表示操作完毕，之后从另外一个核上通过判断这个标记来判定所需要的数据是否已经就绪，这种做法就存在一定风险：标记位先被写入但之前的操作却并未完成(可能是未计算完成，也可能是数据没有从处理器缓存刷新到主存中，最终导致另外的核使用了错误的数据) Java内存模型 JMM规定了线程的工作内存和主内存的交互关系，以及线程之间的可见性和程序的执行顺序。一方面，要为程序员提供足够强的内存可见性保证；另一方面，对编译器和处理器的限制要尽可能地放松。JMM对程序员屏蔽了CPU以及OS内存的使用问题，能够使程序在不同的CPU和OS内存上都能够达到预期的效果。 Java采用内存共享的模式来实现线程之间的通信。编译器和处理器可以对程序进行重排序优化处理，但是需要遵守一些规则，不能随意重排序 Java内存模型—同步八种操作 lock(锁定)：作用于主内存的变量，把一个变量标记为一条线程独占状态 unlock(解锁)：作用于主内存的变量，把一个处于锁定状态的变量释放出来，释放后的变量才可以被其他线程锁定 read(读取)：作用于主内存的变量，把一个变量值从主内存传输到线程的工作内存中，以便随后的load动作使用 load(载入)：作用于工作内存的变量，它把read操作从主内存中得到的变量值放入工作内存的变量副本中 use(使用)：作用于工作内存的变量，把工作内存中的一个变量值传递给执行引擎 assign(赋值)：作用于工作内存的变量，它把一个从执行引擎接收到的值赋给工作内存的变量 store(存储)：作用于工作内存的变量，把工作内存中的一个变量的值传送到主内存中，以便随后的write的操作 write(写入)：作用于工作内存的变量，它把store操作从工作内存中的一个变量的值传送到主内存的变量中 同步规则分析 不允许一个线程无原因地（没有发生过任何assign操作）把数据从工作内存同步会主内存中 一个新的变量只能在主内存中诞生，不允许在工作内存中直接使用一个未被初始化（load或者assign）的变量。即就是对一个变量实施use和store操作之前，必须先自行assign和load操作。 一个变量在同一时刻只允许一条线程对其进行lock操作，但lock操作可以被同一线程重复执行多次，多次执行lock后，只有执行相同次数的unlock操作，变量才会被解锁。lock和unlock必须成对出现。 如果对一个变量执行lock操作，将会清空工作内存中此变量的值，在执行引擎使用这个变量之前需要重新执行load或assign操作初始化变量的值。 如果一个变量事先没有被lock操作锁定，则不允许对它执行unlock操作；也不允许去unlock一个被其他线程锁定的变量。 -对一个变量执行unlock操作之前，必须先把此变量同步到主内存中（执行store和write操作） 并发的优势和风险优势: 速度：同时处理多个请求，响应更快；复杂的操作可以分成多个进程（或线程）同时进行 设计：程序设计在某些情况下更简单，也可以有更多的选择 资源利用：CPU能够等待IO的时候能够做一些其他的事情 风险: 安全性：多个线程共享数据时可能会产生于期望不相符的结果 活跃性：某个操作无法继续进行下去时，就会发生活跃性问题。比如：死锁、饥饿等问题。 性能：线程过多时会使得：CPU频繁切换，调度时间增多；同步机制；消耗过多内存 总结 CPU多级缓存：缓存一致性、乱序执行优化 Java内存模型： JMM规定、抽象结构、同步八种操作以及规则 Java并发的优势与风险]]></content>
      <categories>
        <category>Java并发系列文章</category>
      </categories>
      <tags>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[创建高性能的索引]]></title>
    <url>%2F2019%2F04%2F20%2F%E5%88%9B%E5%BB%BA%E9%AB%98%E6%80%A7%E8%83%BD%E7%9A%84%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[前言本章节来自 高性能MySQL 第五章 创建高性能的索引索引在MySQL中也称为key，它是存储引擎用于快速找到记录的一种数据结构。索引能够轻易将查询性能提高几个数量级 索引基础存储引擎是如何使用索引的？ 首先在索引中找到对应的值，然后根据匹配的索引记录找到对应的数据行。也就是说，如果某个列建立了索引，那么会先在索引中查找。 可创建多个索引，如果多个列，那么顺序非常重要 索引的类型 索引在存储引擎层，因此不同的存储引擎对索引的工作方式也不一样，并不是所有的存储引擎都支持所有类型的索引 B-Tree索引 术语B-Tree是因为MySQL在create table和其他语句中也使用该关键字 内部算法实际使用B+Tree实现，InnoDB使用B+Tree，NDB集群使用T-Tree来存储索引 B-Tree与二叉树的形式存储索引，意味着所有的值都是顺序的，能够快速访问数据不需要进行全表扫描 B-Tree对索引是顺序组织存储，所以适合查找范围数据 哈希索引哈希索引基于哈希表实现，只有精确匹配索引所有列的查询才有效, 哈希索引会将所有的哈希码存储在索引中，同时在哈希表中存储每个数据行的指针 在MySQL中，只有memory引擎支持哈希索引 如果要查询很多关联的表，哈希索引非常使用用于查询 InnoDB有一种特殊的功能 自适应哈希索引 如果它注意到某些索引使用频繁，内部会基于B-Tree之上创建哈希索引 创建自定义索引我们可模拟InnoDB一样创建函数, 虽然还是使用B-Tree进行查找，但是使用哈希值来查询我们有如下表12345CREATE TABLE hash_test( id int unsigned NOT NULL AUTO_INCREMENT PRIMARY KEY , url varchar(255) NOT NULL , url_crc int unsigned NOT NULL DEFAULT 0) 例如url存储”http://www.mysql.com&quot;, 我们进行如下查询效率是非常低的，使用的是全表扫描1SELECT url FROM hash_test WHERE url = 'http://www.mysql.com'; 因此我们在上述表中添加了url_crc它用于存储URL的哈希值，通过哈希值来进行查询，效率是极高的123# 需要将值存储到url_crr列SELECT crc32('http://www.mysql.com');SELECT url FROM hash_test WHERE url = 'http://www.mysql.com' AND url_crc = crc32("http://www.mysql.com"); 上述过程的缺陷是需要手动设置哈希值，当然可使用触发器来完成。注意：哈希冲突问题，或可使用FNV64()哈希函数，冲突比CRC32()少 全文索引它查找的是文本中的关键词，而不是直接比较索引中的值。类似于搜索引擎做的事情 索引的优点 大大减少了服务器需要扫描的数据量 可以帮助服务器避免排序和临时表 可将随机I/O变为顺序IO 索引是最好的解决方案吗?只有当索引帮助存储引擎快速查找到记录带来的好处大于其带来的额外工作时，索引才是最有效的 高性能的索引策略正确的创建和使用索引是实现高性能查询的基础 独立的列不当的使用索引1select id from user where id + 1 = 5 如果查询总的列不是独立的，则MySQL不会使用索引。独立的列是指索引列不能是表达式的一部分，也不能是函数的参数 前缀索引和索引选择性如果需要索引很长的字符串，这会导致索引变的大且慢。我们可采用索引开始部分的字符来提高索引效率。但是也会降低索引的选择性。指定是如果你使用部分字符，那么出现重复的值越高 对于blob、text或很长的varcher类型的列必须采用前缀索引 多列索引在多个列上建立独立的单列索引大部分情况下并不能提高性能，在MySQL5.0版本引入了”索引合并”的策略，一定程度上表上的多个单列索引来定位指定的行 OR、AND、UNION以集合的方式会根据索引查询并不会进行全表扫描 选择合适的索引顺序在B-Tree索引中，意味着安装最左列进行排序，其次第二列 将选择性最高的列放到索引最前列 聚簇索引InnoDB的聚簇索引在同一个结构中保存了B-Tree索引和数据行，它的优点有 把相关数据保存在一起，例如实现电子邮箱时，可根据用户ID聚集数据获取全部邮件 数据访问更快，聚簇索引将索引和数据存储在一个B-Tree中 覆盖索引MySQL可直接获取列的数据，这样就不再需要读取数据行, 如果一个索引包含所有需要查询的字段的值，称为覆盖索引 覆盖索引必须要存储索引的值，并不是所有的存储引擎都支持覆盖索引 使用索引扫描来做排序 MySQL有两种方式生成有序的结果，排序、按索引顺序扫描 可使用explain select table 查看tyep值 如果为index则使用了索引扫描来做排序 压缩（前缀压缩）索引MyISAM使用前缀压缩来减少索引的大小，从而让更多的索引放入内存中 可以在create table语句中指定pack_keys参数来控制压缩索引的方式 冗余和重复的索引如果在相同列上创建多个索引, 那么MySQL需要单独维护重复的索引，这会影响性能 未使用的索引考虑删除 索引和锁索引可以让查询锁定更少的行，如果你的查询从不访问那些不需要的行，那么就会锁定更少的行 InnoDB行锁效率高，只有在访问行的时候才会对其加锁 总结 如果列中建立了索引，存储引擎会根据索引快速查询到记录。 InnoDB使用B-Tree索引，所有中都是顺序的，避免了全表扫描。哈希索引只有memory 可以使用CRC32(‘url’)获取哈希值，可通过哈希值来查询较长的字符串如URL地址，不要使用SHA1()和MD5()作为哈希函数 编写查询语句时尽可能选择合适的索引以避免单行查找，尽可能使用数据原生顺序从而避免而外的排序操作，尽可能使用索引覆盖查询 不要因”应该为where子句中出现的所有列创建索引”所谓的经验法来创建索引，而是要应该根找出消耗最长时间的查询来创建合适的索引]]></content>
      <categories>
        <category>高性能MySQL读书笔记</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[查询性能优化]]></title>
    <url>%2F2019%2F04%2F20%2F%E6%9F%A5%E8%AF%A2%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[前言本章节来自 高性能MySQL 第六章 查询性能优化查询优化、索引优化、库表结构优化需要齐头并进，一个不落，本章节理解MySQL如何真正的指向查询，并明白高效的低效的原因何在 为什么查询速度会慢如果把查询看作是一个任务，那么它由一系列子任务组成，每个子任务都会消耗一定的时间。因此我们要优化查询实际上就是优化子任务。 查询生命周期：从客户，到服务器，然后再服务器上进行解析，生成指向计划，执行，后返回结果给客户端 在每一个消耗大量实际的查询案例中，都可能看到一些不必要的额外操作、重复、某些操作执行太慢等 慢查询基础：优化数据访问查询性能低下最基本的原因是访问数据太多 确认程序是否在检索大量不需要的数据 确认MySQL服务器层是否存在分析大量不需要的数据 是否向数据库请求了不需要的数据有些查询会请求超过实际需要的数据，然后这些多余的数据会被应用程序丢弃。这会给MySQL服务器带来额外的负担，并增加网络开销等 查询不需要的记录我们通常会认为MySQL只会返回需要的数据，实际MySQL是先返回全部结果集后进行计算 多表关联时返回全部列千万不要写select * 这样会导致查询所有关联表的字段数据。应该只取需要的列 总是取出所有列当编写select *时，会让优化器无法完成索引覆盖扫描这类优化。但也并不是坏事，如果你的程序中使用了缓存机制，可能有其好处 重复查询相同的数据如果不断的重复执行相同的查询，每次返回完全相同的数据。比较好的解决方案是，当初次查询的时候将这个数据缓存起来。例如用户每次评论都需要查询用户头像的URL MySQL是否存在扫描额外的记录衡量查询开销的三个指标 响应时间 扫描的行数 返回的行数 它们大致反映了MySQL在内部执行查询时需要访问多少数据，并可大概推算出查询运行的时间。这三个指标都会记录到MySQL的慢日志中 响应时间 响应时间 = 服务时间 + 排队时间服务时间：数据库处理查询真正花了多少时间，排队时间是指服务器因为等待某些资源而没有真正执行查询的时间——可能是I/O、锁等待… 扫描的行数和返回的行数理想情况下扫描的行数和返回的行数应该是相同的。实际情况下这种完美的事并不多 扫描的行数和访问类型在explain语句中的type列反应了访问类型，访问类型有多种，如全表扫描(ALL)、索引扫描、范围扫描等等。这些类型速度慢到快，扫描行数多到少 索引让MySQL以最高效、扫描行数最少的方式找到需要的记录 重构查询的方式一个复杂查询还是多个简单查询设计查询时需要考虑的重要问题是，是否需要将一个复杂的查询分成多个简单的查询。在以前总是认为网络通信，查询解析是一件代价很高的事情 第一种情况不必考虑，现代网络比以前快很多 第二种情况其实在MySQL设计中连接和断开连接的处理非常高效，每秒内部能扫描百万行数据 切分查询有时候对一个大的查询我们可将切分小查询，例如删除旧的数据，将一个大的delete语句切分成多个较小的查询 每次删除数据后，都暂停一会再做下一次删除，这样可以将服务器上原本一次性的压力分散到一个时间很长的时间段中 分散关联查询例如做多表关联查询，有如下语句1234select * from tagjoin tag_post on tag_post.tag.id=tag.idjoin post on tag_post.post_id=post.idwhere tag.tag='mysql' 可以分解为：123select * from tag where tag='mysql'select * from tag_post where tag_id=?select * from post where post.id in(?) 分解关联查询有如下优势： 让缓存效率更高，方便其他程序从缓存中查询单表对象 执行单个查询减少锁的竞争 对数据拆分，做到高性能和可扩展等 查询执行的基础MySQL执行一个查询的流程： 客户端发送一条查询语句给服务器 服务器检查查询缓存，如果缓存中有数据，直接返回。否则进入下一个阶段 服务器进行SQL解析，预处理再由优化器生成对应的执行计划 MySQL根据优化器生成的执行计划，调用存储引擎的API来执行查询 最后将结果返回客户端 我们对其上述的流程进一步的解析 MySQL客户端/服务器通信协议在任何一个时刻，要么由服务器向客户端发送数据，要么客户端向服务器发送数据，这两个动作不能同时发生。 客户端用一个单独的数据包将查询传给服务器，一旦客户端发送了请求，它能做的事就只能等待 相反，服务器响应给客户端的数据通常很多，由多个数据包组成。当开始响应时，客户端必须完整接收整个返回结果，并不能简单的取出前面几条结果（类似拉取数据的过程） 获取结果集看上去是从MySQL服务器获取，实际从函数库(连接MySQL的库函数)的缓存中获取数据 查询状态对于一个MySQL连接，或者说一个线程，任何时刻都由一个连接状态，该状态表示了MySQL当前在做什么。使用SHOW FULL PROCESSLIST命令查询，command列表示当前的状态。有如下状态： sleep：线程正在等待客户端发送新的请求 query：线程正在执行查询或正在将结果发送给客户端 locked：在MySQL服务层，该线程等待表锁 analyzeing and statisticas：线程正在收集存储引擎的统计信息，并生成查询的执行计划 copying to table [on disk]：线程正在执行查询，并将结果集复制到一个临时表中，这种状态通常在做group by union等 sorting result：线程正在对结果集进行排序 sending data：多种情况，线程可能在多个状态之间传送数据，或者在生成结果集等 查询缓存 这里指的是query cache 在解析一个查询语句之前，如果缓存开启，MySQL会优先检查这个查询是否在缓存，是直接返回，否则下一个阶段 查询优化处理此阶段进入查询优化，目标是将一个SQL转换成一个执行计划。MySQL会根据执行计划和存储引擎机械进行交互。这包含了多个子阶段：解析SQL、预处理、优化SQL执行计划，如果该过程有任何错误都可能和终止查询 语法解析器和预处理MySQL会将SQL语句解析为对应的解析树，检查数据表和数据列是否存在、正确等 查询优化器优化器是将SQL转换成执行计划，一条查询可以有多种执行方式，最后都返回相同的值。查询优化器是一个非常复杂的部件，它使用了很多优化策略来生成一个最优的执行计划 查询执行引擎MySQL根据执行计划给出的指令逐步执行，在该过程中，有大量的操作需要通过调用存储引擎实现的接口来完成，这些接口称为’handler API’ 返回结果给客户端最后一个阶段是将结果返回客户端，如果查询可以被缓存，那么MySQL在这个阶段会将结果存放到查询缓存中 查询优化器的提示(hint)如果对优化器选择的执行计划不满意，可使用优化器提供的几个提示(hint)来控制最终的执行计划 sql_small_result和sql_big_resultsql_smqll_result告诉优化器结果集很少，可以将结果集放在内存中的索引临时表，以避免排序操作。sql_big_result则告诉优化器结果集可能非常大，建议使用磁盘临时表做排序操作 p233介绍了更多了的hint 优化特定类型的查询 本节介绍的多数优化技巧都是和特定的版本有关的，对于其他版本未必适用 优化count()count聚合函数：可统计某个列值的数量，也可统计行数 使用count(*)时，并不会像我们猜想的那样扩展成所有的列，实际上，它会忽略所有的列而直接统计所有的行数 如果希望知道的是结果集的行数，最好使用count(*)，这样写意义清晰，性能也会更好 优化group by和distinct使用sql_big_result和sql_small_result来让优化器按照你希望的方式运行 总结 优化数据访问：访问的数据量过大，是否查询了不必要的数据 重构查询方式：将大的查询切分成小的查询，例如将多表查询分成单表查询 理解查询是如何被执行的，以及时间都消耗在那些地方 优化通常需要三管齐下，不做(查询缓存)，少做，快速的做]]></content>
  </entry>
  <entry>
    <title><![CDATA[Schema与数据类型优化]]></title>
    <url>%2F2019%2F04%2F19%2FSchema%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[选择优化的数据类型本章节来自 高性能MySQL 第四章 Schema与数据类型优化 选择正确的数据类型对于获得高性能至关重要 更小的通常更好，尽量使用存储数据的最小数据类型 简单就好，例如整形比字符操作更快，而不是使用字符存储时间 尽量避免NULL，通常情况下最好将列指定为NOT NULL 整数类型存储整数可使用：tinyint、smallint、mediumint、int、bigint。分别使用8、16、24、32、64位存储空间 整数类型有可选的unsigend属性，表示不允许负值 整数计算通常使用bigint 实数类型实数是带有小数部分的数字 float（4字节）和double（8字节）支持浮点运算 decimal用于存储精确的小数, 因为decimal只是一种存储格式，在计算中会转换为double 只在对小数进行精确计算时才使用decimal，例如财务数据 字符串类型不同的存储引擎存储字符的方式可能不同 varchar存储可边长字符串，仅使用必要的空间 char类型是定长的，适合存储更短的字符串。当存储char值时，MySQL会删除所有的末尾空格 binary和varbinary用于存储二进制字符串，存储的是字节码而不是字符 blob和text：存储大数据而设计的字符串类型。前者为二进制，后者为字符串。它们与其他类型不同，存储引擎会做特殊处理，如果值太大，InnoDB会使用专门的”外部”存储区域进行存储 使用枚举类型存储字符串，与数字-字符串映射关系查找表 日期和时间类型MySQL能存储的最小时间颗粒度为秒，MariaDB支持微妙级别的时间类型，可使用bigint存储微秒级别的时间戳 datetime: 可保存大范围的值，1001年-9999年，精度为秒。8字节存储 timestamp: 保存了从1970-1-1午夜（格林尼治标准时间）以来的秒数, 使用4字节存储。 位数据类型位类型，不管底层存储格式和处理方式如何，从技术上来说都是字符串类型 bit：bit(1)定义一个包含单个位的字段, 存储一个或多个true/false set: 存储很多个true/false，以集合的形式来表示 选择标识符整数通常是标识列最好的选择，enum和set只适合标记固定的值，如人的性别、产品类型等，尽量避免字符串类型作为标识列，因为消耗空间随机字符如UUID存储会导致insert、select语句变慢。插入会随机写到索引不同位置，查询逻辑上相邻的行为分布在磁盘和内存分布在不同的位置。 如果存储UUID，移出’-‘，使用unhex()函数转换位16字节数字，存储在binary(16)。检索使用hex()函数格式化位十六进制的格式 schema设计中的陷阱 太多的列 太多的表关联，MySQL限制每个关联操作最多61张表。如果希望查询效率更高，单个查询最好在12个表以内做关联 防止过度的使用enum]]></content>
      <categories>
        <category>高性能MySQL读书笔记</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper分布式锁原理]]></title>
    <url>%2F2019%2F04%2F19%2Fzookeeper%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[死锁 | 活锁概念死锁 如果有两个服务订单服务和商品服务，订单服务需要读取库存操作, 商品服务也需要读取库存操作(图1) 此时订单服务读取完库存之后并没有释放(没有提交)。当商品服务再次读取时，库存数据可能会不一致（脏读）。通常这种情况采用死锁 解决：在订单服务访问库存的时候，无论crud操作都加一把锁(未释放)。那么商品服务就不能访问库存了(图2) 图1 图2 活锁无论多少个服务，只对库存进行读取的操作 那么其他服务都可以去读取，否则都要进行加锁。这种情况成为活锁 分布式锁在分布式环境下，当不同的服务访问某一个共享资源数据(数据库)的时候，可能会发生数据不一致的情况 下订单流程如果在创建订单和扣除库存的操作没有完成（sleep），此时有其他请求进入时，那么库存的数量就可能会造成不一致(之前的值，因为sleep了) zookeeper分布式锁 在获取锁的时候，要根据锁是否被占用才可获取。如果没有占用创建锁 zk临时节点。临时节点会根据客户端会话的断开而断开（释放锁） 总结 什么是死锁：当A服务进行CRUD库存的时候，其他服务不可访问库存 什么是活锁：如果A服务只是进行读库存的操作，那么其他服务可访问库存 利用Zookeeper不能重复创建一个节点的特性来实现一个分布式锁，与redis很类似]]></content>
      <categories>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
        <tag>分布式锁</tag>
        <tag>死锁</tag>
        <tag>活锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper集群]]></title>
    <url>%2F2019%2F04%2F18%2Fzookeeper%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[zookeeper集群概述zookeeper集群包含主从节点，心跳机制（选举模式），架构如下 选举模式: 如果上述图中的master宕机了, slave1和slave进行选举，只有其中一个为master。如果原来的master恢复了，那么会加入到集群中成为slave。集群个数建议为奇数因为需要选举的过程 搭建集群 配置数据文件 myid 1/2/3 对应 server.1/2/3 通过 ./zkCli.sh -server [ip]:[port] 检测集群是否配置成功 拷贝3份zookeeper文件, 在第一份文件中的zoo.cfg配置如下信息12345# 每个文件都需要加入该配置# master节点ip | 数据同步的端口 | 选举端口server.1=192.168.211.136:2888:3888server.2=192.168.211.136:2889:3889server.3=192.168.211.136:2890:3890 在dataDir数据源目录创建 myid 添加数字 1123456[root@zhongjinlang dataDir]# vim myid[root@zhongjinlang dataDir]# ll总用量 8-rw-r--r--. 1 root root 2 4月 18 00:42 myiddrwxr-xr-x. 2 root root 6 4月 18 00:32 version-2-rw-r--r--. 1 root root 4 4月 18 00:32 zookeeper_server.pid 其他文件修改端口 2182、2183和创建myid即可 分别启动 ./zkServer.sh start 并在2181中set一个节点1234567891011[zk: localhost:2181(CONNECTED) 1] create /data 123 Created /data[zk: localhost:2181(CONNECTED) 2] ls /[zookeeper, data]# 退出当前连接，连接2182[zk: localhost:2181(CONNECTED) 3] [root@zhongjinlang bin]# ./zkCli.sh -server localhost:2182# 此时发现从节点2182有了master的数据（2183也有）[zk: localhost:2182(CONNECTED) 0] ls /[zookeeper, data] 查看集群状态12345678[root@zhongjinlang bin]# echo stat | nc localhost 2181Mode: follower # 从节点[root@zhongjinlang bin]# echo stat | nc localhost 2182Mode: leader # 主节点[root@zhongjinlang bin]# echo stat | nc localhost 2183Mode: follower # 从节点 选举模式实验上述可看出2182为主节点，我们将2182进程kill，观察谁会成为主节点 123456789# 此时已经连接不了2182[root@zhongjinlang bin]# echo stat | nc localhost 2182Ncat: Connection refused.# 分别连接其他节点[root@zhongjinlang bin]# echo stat | nc localhost 2181Mode: follower[root@zhongjinlang bin]# echo stat | nc localhost 2183Mode: leader # 2183成为了主节点 总结 zookeeper集群包含主从节点、心跳机制。当主节点挂了，经过选举会将某一个从节点成为主节点，来保证整体服务的高可用]]></content>
      <categories>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeperACL权限控制]]></title>
    <url>%2F2019%2F04%2F18%2FzookeeperACL%E6%9D%83%E9%99%90%E6%8E%A7%E5%88%B6%2F</url>
    <content type="text"><![CDATA[何为ACL(access control lists)?可以对节点设置读写等权限，来保障数据的安全性，权限permission可以指定不同的范围以及角色 ACL命令行 getAcl：获取某个节点的权限信息 setAcl：设置某个节点的权限 addauth： 输入认证授权信息，注册用户将用户根据指定的权限才可登入 ACL的构成一 zookeeper的ACL通过[scheme:id:permission]来构成权限列表 scheme: 代表采用的某种全新机制 id: 代表允许访问的用户 permission: 权限组合字符串 ACL的构成二 - scheme world：只有一个用户, 语法: world:anyone:[permission] 默认情况下它表示任何人都可访问这一节点 auth：认证登入，需要注册用户且有权限。语法：auth:user:password:[permission] digest：需要对密码进行加密才可访问，语法：digest：username:BASE64(SHA1(password)):[permission] ip: 可限制某个ip的访问，语法：ip:192.168.1.1:[permission] super: 代表超级管理员，拥有所有权限 ACL的构成三 - permission权限字符串使用缩写 crdwa，分别表示： c: create创建子节点。表示设置了改权限才可进行创建子节点 r: read获取节点/子节点列表的权限 d: delete删除子节点权限 w: write写节点数据权限 a: admin权限才可分配permission ACL实战world创建一个节点并查看默认的权限123456[zk: localhost:2181(CONNECTED) 18] create /imooc/abc data Created /imooc/abc[zk: localhost:2181(CONNECTED) 19] getAcl /imooc/abc# 拥有所有权限'world,'anyone: cdrwa 设置权限：禁止删除1[zk: localhost:2181(CONNECTED) 20] setAcl /imooc/abc world:anyone:crwa 此时节点的权限、后新创建一个节点再进行删除测试（因为当前节点拥有之前的权限）12345678[zk: localhost:2181(CONNECTED) 21] getAcl /imooc/abc'world,'anyone: crwa[zk: localhost:2181(CONNECTED) 22] create /imooc/abc/xyz new-data-isnotdelete Created /imooc/abc/xyz[zk: localhost:2181(CONNECTED) 23] delete /imooc/abc/xyz# 提示权限不足Authentication is not valid : /imooc/abc/xyz auth明文登入注册账号并登入123456789101112[zk: localhost:2181(CONNECTED) 39] create /name/zhangsan zs Created /name/zhangsan[zk: localhost:2181(CONNECTED) 40] getAcl /name/zhangsan'world,'anyone: cdrwa# 登入提示没有这个用户[zk: localhost:2181(CONNECTED) 41] setAcl /name/zhangsan auth:zhangsan:123:cdrwa Acl is not valid : /name/zhangsan# 注册一个用户[zk: localhost:2181(CONNECTED) 42] addauth digest zhangsan:zhangsan# 再次登入[zk: localhost:2181(CONNECTED) 43] setAcl /name/zhangsan auth:zhangsan:123:cdrwa 此时权限信息123456789101112131415161718192021[zk: localhost:2181(CONNECTED) 44] getAcl /name/zhangsan# 密码存在数据库中是一个密文'digest,'zhangsan:7Tjni+rxBvYp1MDxthriuVT77Gw=: cdrwa``` ## digest使用密文方式登入*创建新的节点 并设置登入账号以及密码*```shell[zk: localhost:2181(CONNECTED) 6] getAcl /name/lisi'world,'anyone: cdrwa# 设置密码（密文）[zk: localhost:2181(CONNECTED) 7] setAcl /name/lisi digest:lisi:7Tjni+rxBvYp1MDxthriuVT77Gw=:cdra# 查看权限[zk: localhost:2181(CONNECTED) 8] getAcl /name/lisi'digest,'lisi:7Tjni+rxBvYp1MDxthriuVT77Gw=: cdra ip控制某个网段是否有权限来访问目录节点，通常用于控制客户端 创建一个新的节点12345678910111213141516[zk: localhost:2181(CONNECTED) 19] create /name/ip ipCreated /name/ip[zk: localhost:2181(CONNECTED) 20] getAcl /name/ip'world,'anyone: cdrwa# 设置权限[zk: localhost:2181(CONNECTED) 21] setAcl /name/ip ip:192.168.117.1:cdrwa[zk: localhost:2181(CONNECTED) 22] getAcl /name/ip'ip,'192.168.117.1: cdrwa# 此时虚拟机不可访问[zk: localhost:2181(CONNECTED) 23] get /name/ipAuthentication is not valid : /name/ip 总结 zookeeper的ACL可对节点进行权限控制来保障数据安全性 world：默认所有用户登入 auth：注册用户登入，注册密码时为明文 digest：注册用户登入，注册密码时为密文 ip: 可限制网段访问 super: 可访问所有权限 ACL使用场景一：开发/测试环境分离，开发者无权操作测试库的节点，只能看 ACL使用场景二：生产环境上控制指定IP的服务可以访问相关节点，防止混乱]]></content>
      <categories>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper的watch机制]]></title>
    <url>%2F2019%2F04%2F18%2Fzookeeper%E7%9A%84watch%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[何为watch?针对每个节点的操作，都会有一个监督者watch。当节点发现变化时，如create、set、delete都会触发一个watch事件。zookeeper中的watch是一次性的，触发后立即销毁。注意是在zookeeper中 父节点，子节点增删改都会触发watch 针对不同类型的操作，触发的watch事件不同：如节点创建事件、节点删除事件、节点数据变化事件 使用watch父节点watch事件 创建父节点触发: NodeCreated 123456789101112# 先设置一个事件（set get都可以 使用help查看）[zk: localhost:2181(CONNECTED) 9] stat /imooc watch Node does not exist: /imooc[zk: localhost:2181(CONNECTED) 10] ls /[zookeeper][zk: localhost:2181(CONNECTED) 11] create /imooc 123# 触发了事件watch::WatchedEvent state:SyncConnected type:NodeCreated path:/imoocCreated /imooc 修改父节点数据触发：NodeDataChanged 删除父节点触发：NodeDeleted 子节点watch事件ls为父节点设置watch 创建子节点触发：NodeChildrenChanged123456789[zk: localhost:2181(CONNECTED) 4] ls /imooc[][zk: localhost:2181(CONNECTED) 5] ls /imooc watch [][zk: localhost:2181(CONNECTED) 7] create /imooc/abc 88 Created /imooc/abcWATCHER::WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/imooc 触发了NodeChildrenChanged 删除子节点也是触发：NodeChildrenChanged 修改子节点不会触发事件 TODO：增加和删除触发的事件为什么一样：因为它们为子节点，子节点和父节点需要进行区分，父节点有不同的事件，对于父节点来说不需要关注子节点的事件。因为父节点只需要告诉客户端 哦我的子节点改变了NodeChildrenChanged事件 总结 watcher使用场景：在集群环境下，如果某个节点发生了变化，例如配置文件的变化，我们可通过watcher监听获取到数据，并将其同步到其他分布式的节点中，达到数据的一致性 zookeeper节点（子父）发生变化时，会触发watch事件，并且出发后立即销毁（在zookeeper中)不同的类型操作watch事件也不同（增、删、改） 在修改节点数据之前可设置watch，如果下次获取该节点时会触发不同的watch类似 父节点触发watch事件有： NodeCreated（增）、NodeDeleted（删）、NodeDataChanged（改） 子节点触发watch事件有: NodeChildrenChanged（增删）、修改不会触发事件因为对于父节点来说并不关心]]></content>
      <categories>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper命令使用]]></title>
    <url>%2F2019%2F04%2F18%2Fzookeeper%E5%91%BD%E4%BB%A4%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[常用命令 通过 ./zkCli.sh打开客户端进行命令行后台操作12[root@zhongjinlang bin]# zkCli.sh[zk: localhost:2181(CONNECTED) 0] ls | ls2查看zookeeper路径1234[zk: localhost:2181(CONNECTED) 0] ls /[zookeeper][zk: localhost:2181(CONNECTED) 2] ls /zookeeper[quota] 查看状态信息123456789101112131415161718192021222324[zk: localhost:2181(CONNECTED) 3] ls2 /[zookeeper]# 创建了之后，zk为这个节点所分配的IDcZxid = 0x0# create-timectime = Thu Jan 01 08:00:00 CST 1970# 修改后zk的IDmZxid = 0x0# 修改后节点的时间mtime = Thu Jan 01 08:00:00 CST 1970# 子节点IDpZxid = 0x0# 子节点发生变化后的version值cversion = -1# 当前节点数据的version，修改后累加1dataVersion = 0# 权限aclVersion = 0# ephemeralOwner = 0x0# 数据长度dataLength = 0# 子节点个数（ls /）numChildren = 1 get | stat stat命令：ls2 = ls + stat get：取出当前节点数据，如果没有数据，与上述内容一至 节点操作命令讲述该节内容前，我们先阐述session的基本原理： 客户端与服务端之间的连接存在会话 每个会话都可设置一个超时时间（30分钟不操作session失效）：心跳结束，则session过期 session过期，临时节点znode会被抛弃 心跳机制： 客户端向服务端的ping包请求 create 创建默认节点节点（持久化节点） 123456789[zk: localhost:2181(CONNECTED) 11] create /imooc immoc-data Created /imooc[zk: localhost:2181(CONNECTED) 12] get /immoc# 当前节点的持久化数据immoc-data..ephemeralOwner = 0x0# 数据长度dataLength = 10 创建节点后目录为： 12[zk: localhost:2181(CONNECTED) 4] ls / [zookeeper, imooc] 创建临时节点: create -e /imooc/tmp immoc-data 创建顺序节点: 每个节点后缀都会增加数值 1234[zk: localhost:2181(CONNECTED) 5] create -s /imooc/sec seq Created /imooc/sec0000000001[zk: localhost:2181(CONNECTED) 6] create -s /imooc/sec seqCreated /imooc/sec0000000002 set | delete set path data [version]：修改节点如果默认不设置版本号，直接删除。加上版本号可实现乐观锁(只能修改最新的数据) delete path [version]: 直接删除节点, 加上版本号只能删除最新数据 四字命令 four letter wordszookeeper可通过它自身提供的简写命令来和服务器进行交互, 可查看服务器的一些状态信息。使用运维来进行监控 需要使用nc命令，安装yum install nc 语法： echo [commod] | nc [ip] [port] stat它可查看zookeeper的状态信息，以及是否mode（集群还是单机） 12345678910111213[root@zhongjinlang /]# echo stat | nc 192.168.211.136 2181Zookeeper version: 3.4.11-37e277162d567b55a07d1755f0b31c32e93c01a0, built on 11/01/2017 18:06 GMTClients: /192.168.211.136:34346[0](queued=0,recved=1,sent=0)Latency min/avg/max: 0/0/27Received: 2696Sent: 2701Connections: 1Outstanding: 0Zxid: 0x45Mode: standaloneNode count: 8 ruok查看当前zk服务是否启动，启动返回imok 12[root@zhongjinlang /]# echo ruok | nc localhost 2181imok[root@zhongjinlang /]# dump列出未经处理的会话和临时节点 12345[root@zhongjinlang /]# echo dump | nc localhost 2181SessionTracker dump:Session Sets (0):ephemeral nodes dump:Sessions with Ephemerals (0): conf查看服务器配置 123456789[root@zhongjinlang /]# echo conf | nc localhost 2181clientPort=2181dataDir=/usr/local/server/zookeeper/zookeeper-3.4.11/bin/../dataDir/version-2dataLogDir=/usr/local/server/zookeeper/zookeeper-3.4.11/bin/../dataLogDir/version-2tickTime=2000maxClientCnxns=60minSessionTimeout=4000maxSessionTimeout=40000serverId=0 cons展示连接到服务器的客户端信息 12[root@zhongjinlang /]# echo cons | nc localhost 2181 /0:0:0:0:0:0:0:1:47224[0](queued=0,recved=1,sent=0) mntr监控zookeeper的健康信息 12345678910111213141516[root@zhongjinlang /]# echo mntr | nc localhost 2181zk_version 3.4.11-37e277162d567b55a07d1755f0b31c32e93c01a0, built on 11/01/2017 18:06 GMTzk_avg_latency 0zk_max_latency 27zk_min_latency 0zk_packets_received 2703zk_packets_sent 2708zk_num_alive_connections 1zk_outstanding_requests 0zk_server_state standalonezk_znode_count 8zk_watch_count 0zk_ephemerals_count 0zk_approximate_data_size 85zk_open_file_descriptor_count 30zk_max_file_descriptor_count 4096 wchs展示watch信息 123[root@zhongjinlang /]# echo wchs | nc localhost 21810 connections watching 0 pathsTotal watches:0 总结 ls / 查看zk目录 ls2 / 查看节点信息 get / 取出当前节点信息 zookeeper存储了节点的各种状态，如数据的大小、修改时间等 ephemeralOwner = 0x0属性如果为0x0那么为临时节点，否则为持久节点（后面是一串数据）。临时节点在断开连接后消失（10秒左右延迟 - 心跳机制） create path data创建持久化节点 set path data [version]修改节点 delete path data [version]删除节点 使用set和delete时候，建议根据version删除，可达到乐观锁的效果，不会删除到之前旧的数据 zookeeper提供四字命令可查看服务器的一些状态信息，语法：echo [命令] | nc [ip] [port]]]></content>
      <categories>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
        <tag>乐观锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper基本数据模型]]></title>
    <url>%2F2019%2F04%2F17%2Fzookeeper%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[zookeeper模型介绍 数据为树形结构，可理解为Linux的目录结构 /usr/local/.. 每一个节点成为znode，它可有子节点，也可以有数据 每个节点分为临时节点和永久节点，临时节点在客户端断开连接后消失。永久节点是一个持久化的节点 每个zk节点都有各自的版本号，可通过命令显示节点信息 每当节点数据发生变化时，那么该节点的版本号就会累加（乐观锁） 删除/修改过时的节点，版本号不匹配则会报错 每一个zk节点存储的数据不易过大，几k即可 节点可设置权限ACL，可通过权限来限制用户访问 数据模型基本操作 客户端连接 - 查看znode结构 - 关闭客户端连接 连接客户端123456[root@zhongjinlang bin]# ./zkCli.sh# 提示一推信息...WATCHER::WatchedEvent state:SyncConnected type:None path:null# 这里需要回车[zk: localhost:2181(CONNECTED) 0] zookeeper命令 查看znode12[zk: localhost:2181(CONNECTED) 4] ls /zookeeper/quota[] zookeeper的作用 master节点选举，当主节点挂了，从节点就会接手工作，从而保证集群是高可用的 统一配置文件管理，即只需要部署一台服务器，则可以把相同的配置文件同步更新到其他所有服务器，例如在一个集群的生产环境下，修改了一个redis的配置那么其他服务都需要进行修改，这样就很麻烦 发布订阅模式，类似消息队列MQ, dubbo发布者把数据数据存储到znode节点上，订阅者会读取这个数据 提供分布式锁，分布式环境中不同进程之间争夺资源，类似于多线程中的锁 当A处理完B、C、D 集群管理，集群中保证数据的强一致性 总结 zookeeper的结构为树形结构，节点分为临时节点和永久节点。前者会在客户端断开后消失，后者是一个持久化的节点 zookeeper可保证集群高可用、提供了分布式锁保证数据一致性、可以统一管理集群中的配置文件进行同步]]></content>
      <categories>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
        <tag>分布式锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper概述]]></title>
    <url>%2F2019%2F04%2F17%2Fzookeeper%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[简介ZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务，是Google的Chubby一个开源的实现，它是集群的管理者，监视着集群中各个节点的状态根据节点提交的反馈进行下一步合理操作。最终，将简单易用的接口和性能高效、功能稳定的系统提供给用户 分布式系统 很多台计算机组成一个整体，一个整体一致对外并处理同一个请求 内部的每台计算机都可相互通信（rest/rpc） 客户端到服务端的一次请求到响应结束会历经多台计算机 zookeeper特性 一致性：数据一致性，将数据按照顺序分批入库 原子性： 事务要么成功要么失败，不会局部化 单一视图： 客户端连接集群中的任一zookeeper节点，数据都是一致的 可靠性： 每次对zookeeper的操作状态都会保存在服务端 实时性: 客户端可以读取到zookeeper服务端的最新数据 zookeeper的安装JDK依赖 安装解压好后配置环境变量，java-version查看是否安装成功 12345# set java# java安装路径export JAVA_HOME=/usr/local/server/java/jdk1.8export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport PATH=$PATH:$JAVA_HOME/bin zookeeper配置 下载地址：https://archive.apache.org/dist/zookeeper/ 安装解压后配置环境变量 1234export JAVA_HOME=/usr/local/server/java/jdk1.8export ZOOKEEPER_HOME=/usr/local/server/zookeeper/zookeeper-3.4.11export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar export PATH=$PATH:$ZOOKEEPER_HOME/bin:JAVA_HOME/bin 核心配置文件123456789101112#用于计算时间的单元, 比如session超时（N * tickTime）tickTime: #用于集群，允许从节点连接并同步到master节点的初始化连接时间，以tickTime的倍数来表示initLimit: #用于集群，master主节点与从节点之间的消息通信，请求和答应的时间（心跳机制）syncLimit:#必须要配置的。用来zookeeper存储的数据(手动创建) dataDir: #日志目录，如果不配置会和data目录公用（手动创建）dataLogDir: #连接服务器的端口，默认2181clientPort: 启动 ./zkServer.sh 命令帮助 ./zkServer.sh start 启动1234[root@zhongjinlang bin]# ./zkServer.sh startZooKeeper JMX enabled by defaultUsing config: /usr/local/server/zookeeper/zookeeper-3.4.11/bin/../conf/zoo.cfgStarting zookeeper ... STARTED 查看启动状态 ./zkServer.sh status12345[root@zhongjinlang bin]# ./zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /usr/local/server/zookeeper/zookeeper-3.4.11/bin/../conf/zoo.cfg# 表示单机状态Mode: standalone 总结 zookeeper用于提供协调服务，作用与分布式，发挥其优势，可为大数据服务 一次请求历经了多台服务器，这是分布式系统的最简单 zookeeper协调服务可理解为交通堵塞时交警的作用]]></content>
      <categories>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis缓存使用与优化]]></title>
    <url>%2F2019%2F04%2F16%2Fredis%E7%BC%93%E5%AD%98%E4%BD%BF%E7%94%A8%E4%B8%8E%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[缓存的收益与成本收益 加速读写：CPU L1/L2/L3 Cache、Linux page Cache加速硬盘读写、浏览器读写、Ehcache缓存数据库结果 降低后端负载：降低MySQL的负载等 成本 数据不一致：缓存层和数据层有时间窗口不一致，和更新策略有关。也就是说，需要将数据库中的数据存入redis进行缓存，如果数据库更新了，那么缓存如何更新呢? 代码维护成本高： 多了一层缓存逻辑 运维成本：例如redis-cluster，或者使用云服务 使用场景 降低后端负载：对高消耗的SQL, 如join结果集 / 分组结果集 加速请求响应：优化IO响应时间 大量写合并为批量写： 计数器先在redis累加，再批量写入DB 缓存更新策略当数据库数据更新时，那么缓存将如何进行更新维护呢？ 先进先出算法剔除：例如maxmemory-policy(最大内存限制)，当超过指定值之后，删除过期的key。不需担心每一个key是如何删除的。适用场景：控制内存 超时剔除：设置过期时间expire。适用场景：存储不是很重要的数据 主动更新：开发控制生命周期，由自己来控制每一个key的更新周期 开发建议： 超时剔除 + 主动更新 缓存颗粒控制通常我们都是这样设计一个缓存系统的：先从redis查询数据，如果有直接返回。如果没有则从数据库中查询然后再加入缓存中。伪代码如下：123set userid 'select * from user where id=1'set userid 'select column.. from user where id=1' 那么到底要缓存select * 还是部分字段的数据？该问题为缓存粒度问题 缓存粒度控制 - 三个角度 通用性：全部属性更好 占用空间：部分数据更好 代码维护：表面上全部属性更好 生产环境通常采用部分属性，缓存需要考虑到性能问题、序列化问题 缓存穿透问题通常情况下缓存设计是这样的：第一次访问redis没有数据就去从数据库中查询，然后将数据加入缓存中，当下此访问redis时就直接从缓存中获取了。这是通常的情况下，如果第二个步骤访问数据库没有数据会怎样？如下如图结构： 此时就失去了缓存的意义，因为缓存就是用来保存持久层。 原因 业务代码问题： 本身你编写的接口就拿不到持久层数据或是调用别的接口拿不到持久层数据 恶意攻击、爬虫等等: 根据URI一定的规则访问到接口 如何发现 业务的相应时间 业务本身问题 相关指标：总调用数、缓存层命中数、存储层命中数 解决缓存空对象如果从数据库中查询的值不存在，进行一个判断直接将null就作为’数据’存入缓存中 但是这种解决方案还是有问题，持久层和缓存层数据”短期”不一致。比如业务上的接口问题调用时可能出现网络原因拿不到结果，此时你将它作为null缓存了。如果恢复了，此时缓存是一个null的状态 伪代码12345678910111213141516// 从redis中取数据public String getValueByRedis(String key)&#123; String cacheValue = cache.get(key); if(StringUtils.isBlank(cacheValue) )&#123; // 为空从持久层获取数据 String storageValue = storage.get(key); // 加入缓存 jedis.set(key, storageValue); // 防止缓存穿透, 如果数据库值真为空，设置过期时间 if(StringUtils.isBlank(storageValue) )&#123; jedis.expire(key, 60 * 5); &#125; &#125;else&#123; return cacheValue; &#125;&#125; 布隆过滤器拦截通过很少的内存对数据的过滤, 例如在一个电话本中判断一个电话是否在电话本中，通常不会将电话本存储到内存中，占用内存可能会很大。布隆过滤器就是解决这种类似的问题，它通过一些算法，将电话本存入过滤器，当要判断电话是否在电话本中时，通过很少的算法来进行解决 布隆过滤器用于检索一个元素是否在一个集合中 缓存无底洞问题 2010年，Facebook有3000个memcache节点 发现问题： 加机器性能没有提升反而下降 问题关键点 更多的j机器 != 更高的性能 批量接口需求（mget、mset等） 数据增长与水平扩展需求 缓存雪崩问题当缓存服务承载大量请求，如果缓存服务异常/宕机，流量直接压向后端DB，造成级联故障 优化方案 保证缓存高可用：个别节点、个别机器、甚至是机房 依赖隔离组件为后端限流 提前演练：压力测试 cache服务高可用：redis-sentinel、redis-cluster、redis-VIP 热点key重建优化缓存重建过程：我们知道使用缓存首先从缓存中获取,如果获取不到从数据库中获取，如果获取到了将数据写入缓存，该过程为缓存的重建的过程。该过程可能会出现的问题 热点key（访问量大） + 较长的重建时间： 大量的线程作查询数据源和缓存重建的工作。解决方案： 互斥锁第一个获取缓存的线程需要做重建的时候，将重建过程加锁。完成了重建工作再将锁解开。在这期间其他线程的发现重建过程处于等待状态，直到最后一个发现锁解开就可直接获取缓存进行输出 该解决方案没有大量的重建过程，但有等待的问题 伪代码12345678910111213141516171819202122// 从redis中取数据public String getValueByRedis(String key)&#123; String cacheValue = cache.get(key; if(cacheValue == null)&#123; // 重建工作 // 设置互斥锁key String mutexKey = "mutexKey:" + key; //setnx命令：key不存在才进行设置 if(redis.set(mutexKey, "1", "ex 180", "nx") )&#123; // 开始重建 value = db.get(key); redis.set(key, value); // 删除锁 redis.delete(mutexKey); &#125;else&#123; // 存在该key，让其他线程休眠 Thread.sleep(50); // 再次尝试获取key get(key); &#125; &#125; return cacheValue; &#125; 永不过期 缓存层面： 没有设置过期时间 功能层面： 为每一个value添加逻辑过期时间，发现过期了，使用单独线程去完成缓存的重建工作 该方案相比互斥锁没有等待的过程 总结 缓存的收益: 加速读写、降低后端存储负载 缓存成本: 缓存和存储数据不一致性、代码维护成本、运维成本 缓存更新策略：超时删除 + 主动更新 缓存穿透问题：使用缓存空对象和布隆过滤器解决，注意他们各自使用场景 缓存雪崩问题： 缓存层的高可用、客户端降级、提前压力测试 热点key重建问题： 互斥锁、”永不过期” 来解决]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL架构与存储引擎]]></title>
    <url>%2F2019%2F04%2F15%2FMySQL%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[MySQL逻辑架构本章节来自 高性能MySQL 第一章 MySQL架构与历史 最上层服务是多数基于网络的客户端/服务器都有类似的架构 第二层是MySQL核心服务功能，查询解析、分析、优化、缓存、内置函数、存储过程、触发器、视图等 第三层包含存储存储引擎，它负责MySQL中数据的存储和提取，存储引擎API包含几十个底层函数，例如执行开始一个事务等操作。注意：存储引擎并不会提取SQL MySQL的存储引擎InnoDBInnoDB是mysql的默认事务型引擎, 使用最为广泛的的存储引擎。它的作用是处理大量的短期事务，短期事务大部分情况是正常提交的，很少会被回滚。 MyISAM在MySQL5.1及之前的版本，默认使用MyISAM存储引擎。它提供了全文索引、压缩、空间函数等特性，它不支持事务和行级锁。不要默认开启此引擎，应当使用InnoDB。 MySQL内建的其他存储引擎Archive它只支持insert、select操作，archive引擎会缓存所有的写并利用zlib对插入的行进行压缩，比MyISAM更少的IO Blackhole它没有实现任何存储机制，它会丢弃所有插入的数据，不做任何保存。可用于复制数据库进行备份，可在复制架构和日志审核时发挥作用。但是并不推荐 CSV它可将普通的CSV文件（逗号分隔的文件）作为MySQL的表来处理，但这种表不支持索引。可将excel中的数据存储为CSV文件然后复制到MySQL数据目录。它可作为一种数据交换的机制 Federated该引擎是访问其他MySQL服务器的一个代理，默认是禁用的，它有一个后续的版本为FederatedX Memory如果需要快速访问数据，且这些数据不会被修改，它把数据保存在内存, 重启之后表结构还存在但是数据会丢失。使用场景： 查询或映射表 缓存周期性聚合数据的结果 保存数据分析中产生的中间数据 memory支持hash索引，因此查询很快 MergeMyISAM的变种引擎，由多个表合并出来的虚拟表，该引擎已被放弃 NDB当时的MySQL AB公司从索尼爱立信公司收购了NDB数据库，开发了自己的NDB集群存储引擎 第三方引擎MySQL从2007年提供了插件式的存储引擎API，因此由大多数的第三方产品或开源项目 OLTP类引擎InnoDB是该类型的引擎，还有其他比如支持事务和MVCC的其中一个由PBXT 面向列的存储引擎MySQL默认是面向行的，没一行的数据是一起存储的，查询也是以行为单位处理。而在大数据量处理时，面向队列方式可能效率更高infobright 是一种面向列的存储引擎，在非常大的数据量（数十TB）时，该引擎工作良好。它为数据分析和数据仓库而设计。该引擎不支持索引，不过在这么大的数据量，索引也很难发挥作用 社区存储引擎社区所提供的存储引擎很多，这里只介绍常见的 Groonga： 全文索引引擎，号称可以提供准确高效的全文索引 OQGraph: 支持图操作，比如最短路径问题，用SQL很难实现该问题 Q4M：该引擎在MySQL内部实现了队列操作，用SQL很难实现该问题 SphinxSE: 该引擎为Sphinx全文索引搜索服务器提供了SQL接口 Spider: 该引擎可将数据切分为不同的分区，高效透明的实现了分片，并可分片执行并行查询 VPForMySQL: 该引擎支持垂直分区，指的是将表分成不同列的组合，进行单独存储。但是对于查询来说，看到的还是一张表 选择合适的引擎多数情况下，innoDB都是正确的选择, Oracle在MySQL5.5将innoDB作为默认引擎。如何选择归纳一句话 除非要用到innoDB不具备的特性，并且没有其他办法可替代 , 如果需要事务，InnoDB是目前最稳定的选择 如果不需要要事务，并主要是SELECT和INSERT那么使用MyISAM 转换表的引擎每种方法都有优缺点，以下只讲述三种方法： alter table1ALTER TABLE mytable ENGINE = InnoDB 上诉语法可适用任何存储引擎，但有一个问题：需要执行很长时间，MySQL会按行将数据从原表复制一张新的表中，在复制期间可能会消耗系统所有的IO资源，同时原来的表会加上读锁 导出于导入 创建于查询（create、select） 总结 MySQL拥有分层架构，上层是服务器层的服务、和查询执行引擎。下层存储引擎 mysql的引擎和区别：InnoDB、MyISAM, 5.1之前的版本采用MyIsAM。InnoDB支持事务而MyISAM不支持事务，如果需要大量的插入数据操作使用MyISAM，其他情况优先使用InnoDB 如果要处理大数据量使用第三方引擎 infobright]]></content>
      <categories>
        <category>高性能MySQL读书笔记</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis集群总结]]></title>
    <url>%2F2019%2F04%2F15%2Fredis%E9%9B%86%E7%BE%A4%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[集群总结redis集群高可用实现方式：首先我们知道如果A节点如果发送故障，那么期望它的从节点来接管主节点的任务，也就是说可以进行读写服务 那么从节点的数据怎么和主节点的数据达到一致性呢？首先从节点拿到数据的原理是通过RDB传输(主从复制原理) 的方式来获取数据，而主从数据的一致性通过偏移量来判断数据的一致性。 数据一致性问题解决了，而故障转移redis是如何做到的？ 官方的两种方案： redis-sentinel 和 redis-cluster，redis-sentinel（哨兵）是一个独立的程序，它的作用是监控多个主从节点，如果发现主节点挂了会进行内部选举的模式让一个从节点成为主节点。而redis-cluster专门用于搭建集群模式的情况下使用，因此它本身就具备了高可用的特性。 rediscluster总结 rediscluster数据分区规则使用虚拟槽（16384），每个节点负责一部分槽和相关数据，实现数据的和请求的均衡负载 搭建集群步骤：准备节点、节点握手、分配槽、复制。redis.trib.rb工具用于快速搭建集群 集群伸缩实现是通过节点之间移动槽和相关数据的实现 加入：把槽从原来的节点迁移到新节点 退出：如果要退出集群的节点中槽有数据，那么将它迁移到其他节点，在通过cluster forget命令广播集群让所有节点忘记 集群故障转移过程为故障发现和节点恢复。节点的下线分为主观下线和客观下线，当半数主节点认为你是故障了标记为客观下线。从节点负责故障恢复的过程，保证集群可用性]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rediscluster常见运维问题]]></title>
    <url>%2F2019%2F04%2F15%2Frediscluster%E5%B8%B8%E8%A7%81%E8%BF%90%E7%BB%B4%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[前言本章讲述搭建rediscluster后常见的问题，理解这些问题，使我们对redis分布式集群的架构有一定的帮助 集群完整性也就是我们在rediscluster篇章中配置的 cluster-require-full-coverage 默认true它表示是否需要集群中的所有节点都是一个在线的状态，所有的0-16384槽都在一个服务的状态，才认为整个集群是完整的，才会对外提供服务 但是对于大多数的业务都无法容忍，如果我们有1000主从节点，当其中一节点进行故障转移的期间（clusterdown the cluster is down）那么此时整个集群就不可用了 在实际生产过程中设置为 no 为何默认为yes？ 集群中16384个槽全部不可用，这也设计就是为了保证集群完整性 带宽消耗我们知道集群节点之间使用信息的交换(gossip)，所以一定有带宽的开销。官方建议：最多1000个节点当节点规模较大时候，会有不容忽视的带宽消耗 消息发送的频率：节点发现与其他节点最后通信时间超过cluster-node-timeout/2时会直接发送ping消息 消息数据量：消息会槽信息，槽数组(2kb)，集群的1/10状态数据(10个节点1kb) 集群规模越高带宽越高 优化： cluster-node-timeout带宽和故障转移的速度的均衡 发布/订阅广播类似于mq的生产消息于订阅消息。当某个节点要通知集群中其他节点的时候，例如故障恢复。其他节点就会收到该消息，这样会产生一个问题，节点的带宽开销会很大。 publish在集群每个节点广播，会加重带宽 解决: 如果只需要做到高可用，单独使用redissentinel 数据倾斜如果将原来节点的数据分布到多个节点上，可能会产生数据的倾斜，有如下原因 节点和槽分配不均匀 不同槽对应键值数量差距大 包含bigkey 内存相关配置不一致 读写分离 只读连接：集群模式的从节点不接受任何读写请求，会跳转到数据对应槽的主节点上，如果需要读（每次客户端执行readonly） 读写分离跟为复杂不建议在集群模式下实现，需要考虑复制延迟、读取过期数据、从节点故障。如果真要实现需要维护slave（思路类似redis-sentinel） 数据迁移如果我们需要将原来的单机节点迁移到rediscluster，这种模式如何实现？ redis-trib.rb import可将单节点数据迁移到集群。不支持在线迁移，b不支持断点续传，单线程迁移 在线迁移：唯品会&gt;redis-migrate-tool、豌豆荚&gt;redis-port 集群VS单机 集群批量操作有限：如mget、mset必须在一个槽 集群事务和lua无法跨节点使用 集群模式只有一个db0，没有16个数据库 集群模式复制只支持一层，不支持树形复制结构 rediscluster满足容量和新能的扩展性，但很多业务达不到一定的QPS，很多场景下redis-sentinel已经足够好]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rediscluster高可用]]></title>
    <url>%2F2019%2F04%2F15%2Frediscluster%E9%AB%98%E5%8F%AF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[故障转移rediscluster不需要到sentinel即可完成故障转移，实现高可用的集群特性。它与sentinel很相似，分为故障发现和故障恢复的过程 故障发现依赖节点之间的通信使用ping/pong消息实现故障的发现，不需要sentinel 主观下线定义：某个节点认为另一个节点不可用，”偏见”，代表一个节点对另一个节点的认知流程： 当某个节点断开连接之后，超过node-timeout超时时间，那么标记为pfaill 客观下线定义： 持有半数以上的槽的节点都标记某个节点时，就认为那个节点不可用流程： 记录每个节点的状态，如果状态不可用，会将信息添加到故障列表中，列表用于维护故障列表，后再尝试客观下线。之后通知集群内所有节点标记故障节点为客观下线，通知故障节点的从节点准备执行转移 故障恢复客观下线通知从节点，从节点接收到消息开始准备进行故障的恢复，从而保证集群的高可用，实现流程有： 资格检查检查哪些从节点可能成为主节点的资格条件： 每个从节点检查与故障主节点的断线时间 超过cluster-node-timeout(15) * cluster-slave-validity-factory(10)取消资格 选举具备更大的偏移量（更接近主节点）成为主节点的可能性更大，选举的时间更小且投票个数更高，收集3张选票大于N/2+1，那么执行替换主节点工作 替换主节点 当前节点取消从节点复制，变为主节点（slaveof one one） 执行clusterDelSlot撤销故障主节点负责的槽，并执行clusterAddSlot把这些槽分配给自己 集群广播消息，表明已经替换了故障从节点 故障转移实验环境: springboot + 3主3从节点目的: 不断的从节点中set值, 中途将其中一个master kill观察情况 maven依赖和配置1234567891011121314151617&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;io.lettuce&lt;/groupId&gt; &lt;artifactId&gt;lettuce-core&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt;&lt;/dependency&gt; TODO: spring-boot-starter-data-redis内部使用的不是jedis实现的连接池，经过之前的测试，如果使用lettuce-core这个连接池没有达到故障转移的效果, 所以这次实验使用jedis内部的连接池完成 application.yml12345678910spring: redis: cluster: nodes: - 192.168.211.136:7001 - 192.168.211.136:7002 - 192.168.211.136:7003 - 192.168.211.136:7004 - 192.168.211.136:7005 - 192.168.211.136:7006 测试类12345678910111213141516171819202122232425262728@RunWith(SpringRunner.class)@SpringBootTestpublic class SpringbootRedisApplicationTests &#123; @Autowired private RedisTemplate redisTemplate; private Logger logger = LoggerFactory.getLogger(SpringbootRedisApplicationTests.class); @Test public void contextLoads() &#123; while (true) &#123; try &#123; // 不断的set值 int index = new Random().nextInt(10000); String key = "k-" + index; String value = "v-" + index; redisTemplate.boundValueOps(key).set(value); // 输出日志 logger.info("&#123;&#125; value is &#123;&#125;", key, redisTemplate.boundValueOps(key).get()); Thread.sleep(500); &#125; catch (Exception e) &#123; logger.error(e.getMessage(), e); &#125; &#125; &#125;&#125; 此时节点状态： 开始不断的set值，在进行kill主节点，观察日志输出：]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[springboot原理]]></title>
    <url>%2F2019%2F04%2F14%2Fspringboot%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[前言Spring Boot是Spring旗下众多的子项目之一。其理念是约定优于配置，它通过实现了自动配置（大多数用户平时习惯设置的配置作为默认配置）的功能来为用户快速构建出标准化的应用。内置了嵌入式的Tomcat、Jetty等Servlet容器，应用可以不用打包成War格式，而是可以直接以Jar格式运行 特点：spring boot不是对spring的增强，而是提供了一种快速使用spring的方式 核心：起步依赖、自动配置 快速搭建使用idea快速搭建一个springboot项目： 创建springboot项目 指定包名和项目名 选择需要的依赖文件 项目结构如下 pom依赖文件如下 编写一个controller 123456789@RestController@RequestMapping("/user")public class UserController &#123; @RequestMapping("/info.do") public String info() &#123; return "这是我的第一个spring boot程序"; &#125;&#125; 启动spring boot引导类 123456@SpringBootApplicationpublic class SpringBootDemoApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringBootDemoApplication.class, args); &#125;&#125; 访问8080 原理起步依赖起步依赖是spring boot的核心功能之一，起步依赖本质是maven定义了对其他库的传递依赖，简单的说，起步依赖就是将具备某种功能的坐标打包在一起，并提供一些默认的功能 springboot的起步依赖核心坐标123456&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.4.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;&lt;/parent&gt; 它提供了大量的Maven默认依赖。使用它之后，常用的包依赖可以省去version标签 自动配置spring boot默认配置了一些用户常用的配置，因此不需要我们先编写xml（如web.xml）在编写代码进行开发，那么它是如何做到的呢? 我们开发一个spring boot都会有如下启动类123456@SpringBootApplicationpublic class SpringBootDemoApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringBootDemoApplication.class, args); &#125;&#125; SpringBootApplication注解结构 @SpringBootConfiguration: 继承了Configuration（可进入查看），作用是取代xml的方式(spring的纯注解开发) @ComponentScan：组件扫描，表示其他包可使用spring组件 EnableAutoConfiguration注解:开启springboot的注解功能，springboot的四大神器之一，其借助@import的帮助，将所有符合自动配置条件的bean定义加载到IOC容器中。EnableAutoConfiguration有如下信息 AutoConfigurationImportSelector类该类用于管理加载默认的配置信息，例如我们使用springmvc必须会经过dispatchservlet，那么spring boot会将这些配置存储到如下文件中：可发现这些类都AutoConfiguraion作为结尾, spring boot会将这些存有自动配置的类获取并加载, 我们分析其源代码：可发现@bean，作用是取代xml的bean标签(spring的纯注解开发) EnableConfigurationProperties注解该注解的作用是加载ServerProperties服务器相关配置属性类，例如上述搭建的程序，访问tomcat默认端口为8080，这些默认配置存储在如下文件中： 当需要覆盖默认配置，在application.xml中指定即可 总结springboot的启动类会加载SpringBootApplication注解，该注解包含了@configuration（该类为配置类，取代xml）、@import（加入其他配置类）、自动装配的核心注解EnableAutoConfiguration，它用于完成记录默认的配置信息（web.xml）、服务器的配置信息(tomcat)]]></content>
      <categories>
        <category>springboot</category>
      </categories>
      <tags>
        <tag>springboot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis集群伸缩]]></title>
    <url>%2F2019%2F04%2F12%2Fredis%E9%9B%86%E7%BE%A4%E4%BC%B8%E7%BC%A9%2F</url>
    <content type="text"><![CDATA[引入上一篇章中，我们分析了节点取余和一致性哈希产生数据迁移的问题，而rediscluster并不会有这种情况，因为每个节点负责槽的范围都是固定的，每个节点与槽的分配都具备权力 集群伸缩原理它的原理其实就是rediscluster的基本架构（在rediscluster文章所讲述）： 节点: 每个节点负责读写 meet: 完成节点之间通信工作 A meet C; A meet B; B &gt; C 指派槽: 只有给节点指派了槽才可以进行读写 复制: 为了达到高可用，有主从复制概念 集群伸缩 = 槽和数据在节点之间的移动 集群扩容首先加入集群的作用有：实现扩容、作为从节点负责故障转移，实现扩容的步骤： 准备新的节点 集群模式 配置和其他节点统一 启动后是孤儿节点 加入集群（meet）将两个孤儿节点加入到集群中，(可通过cluster-meet完成，不推荐)，推荐使用redis-trib.rb加入集群，它会检测你加入的节点是否为孤儿节点，是才可进行加入 迁移槽和数据 把其他节点负责的槽均匀的迁移到新节点，让它工作起来 槽迁移规划 我们需要将槽进行均衡的分配 迁移数据 让目标节点准备导入槽的数据: cluster setslot {slot} importing {sourceNodeId} 对源节迁移出槽数据：cluster setslot {slot} migrating {targetNodeId} 对源节点循环遍历槽，获取count个槽的key：cluster getkeysinslot {slot} {count} 在源节点执行：migrate {targetIp} {targetPort} key 0 {timeout} 重复执行步骤3-4直到槽下所有的键数据迁移到目标节点 通知槽分配个目标节点：cluster setslot {slot} node {targetNodeId} 加入集群实战 此时的集群状态 12345678910111213141516171819202122232425262728293031323334353637# 启动7006和7007(拷贝文件和修改配置文件不在阐述)root 52263 1 0 05:10 ? 00:00:00 redis-server *:7006 [cluster]root 52272 1 1 05:10 ? 00:00:00 redis-server *:7007 [cluster]# 此时为孤立状态[root@zhongjinlang redis]# redis-cli -p 7006127.0.0.1:7006&gt; cluster nodesd65a5d7b0dcc09542761ac746ab4058fd002a5a4 :7006 myself,master - 0 0 0 connected# 加入集群[root@zhongjinlang redis]# redis-cli -p 7000127.0.0.1:7000&gt; cluster meet 192.168.211.136 7006127.0.0.1:7000&gt; cluster meet 192.168.211.136 7007127.0.0.1:7000&gt; cluster nodes# 此时已经加入了集群0936d215f3adc8aad93f935ba5ea72c7d6cc7bd0 192.168.211.136:7007 master - 0 1555017424796 7 connectedd65a5d7b0dcc09542761ac746ab4058fd002a5a4 192.168.211.136:7006 master - 0 1555017423368 0 connected# 7007（从）复制 7006（主）配置[root@zhongjinlang redis]# redis-cli -p 7007 cluster replicate d65a5d7b0dcc09542761ac746ab4058fd002a5a4OK# 为7006主节点分配槽，在/usr/local/bin[root@zhongjinlang bin]# redis-trib reshard 192.168.211.136:7000# 提示输入How many slots do you want to move (from 1 to 16384)? 4096What is the receiving node ID? d65a5d7b0dcc09542761ac746ab4058fd002a5a4Please enter all the source node IDs. Type 'all' to use all the nodes as source nodes for the hash slots. Type 'done' once you entered all the source nodes IDs.Source node #1:all# 查看槽信息[root@zhongjinlang bin]# redis-cli -p 7006 cluster nodes# 可发现槽有三份数据d65a5d7b0dcc09542761ac746ab4058fd002a5a4 192.168.211.136:7006 myself,master - 0 0 8 connected 0-1364 5461-6826 10923-12287 收缩集群收缩集群就是对一个节点进行下线，实现步骤如下： 下线迁移槽我们将下线7006和7007，并将他们的槽均匀的给其他节点 1234567891011121314151617181920212223242526# 查看集群redis-cli -p 7000 cluster nodes# 将7006节点的槽（有三段）迁移给主节点7000[root@zhongjinlang bin]# redis-trib reshard --from d65a5d7b0dcc09542761ac746ab4058fd002a5a4 --to 5fe9ca13ded00d87f3271380f9e0c7278ec8c483 --slots 1366 192.168.211.136:7006# 迁移给7001[root@zhongjinlang bin]# redis-trib reshard --from d65a5d7b0dcc09542761ac746ab4058fd002a5a4 --to d0941ed132c5a495545e1e30808cefd243942309 --slots 1365 192.168.211.136:7006# 迁移给7002[root@zhongjinlang bin]# redis-trib reshard --from d65a5d7b0dcc09542761ac746ab4058fd002a5a4 --to 3f3b1a02189aa7b27d5ebc2038a07441933b7327 --slots 1365 192.168.211.136:7006# 先下线7006的从节点7000[root@zhongjinlang bin]# redis-trib del-node 192.168.211.136:7000 0936d215f3adc8aad93f935ba5ea72c7d6cc7bd0...&gt;&gt;&gt; Sending CLUSTER FORGET messages to the cluster...&gt;&gt;&gt; SHUTDOWN the node.# 下线7006[root@zhongjinlang bin]# redis-trib del-node 192.168.211.136:7000 d65a5d7b0dcc09542761ac746ab4058fd002a5a4# 此时集群状态[root@zhongjinlang bin]# redis-cli -p 7000 cluster nodesbf5e78c9bc9728a2af1ac4077a539207146963a9 192.168.211.136:7004 slave d0941ed132c5a495545e1e30808cefd243942309 0 1555021435868 10 connected3a959f75d04e953b71c80f74f4d5c63b9fd319fd 192.168.211.136:7003 slave 5fe9ca13ded00d87f3271380f9e0c7278ec8c483 0 1555021438977 9 connected3f3b1a02189aa7b27d5ebc2038a07441933b7327 192.168.211.136:7002 master - 0 1555021437945 11 connected 10924-163835fe9ca13ded00d87f3271380f9e0c7278ec8c483 192.168.211.136:7000 myself,master - 0 0 9 connected 0-5461d0941ed132c5a495545e1e30808cefd243942309 192.168.211.136:7001 master - 0 1555021440005 10 connected 5462-10923a9b8f471b99a598cf295f3c15dcb12325302d1bd 192.168.211.136:7005 slave 3f3b1a02189aa7b27d5ebc2038a07441933b7327 0 1555021434839 11 connected rediscluster客户端当我们对一个rediscluster执行set或get时，rediscluster的计算规则是怎样的? move重定向 当客户端拿到moved异常后，需要对目标执行命令，注意此时客户端并不会自动找到目标节点进行重定向发送(如果使用redis-cli)。而redis-cli-c内部完成了重定向 槽命中和不命中情况计算某个key所对应的槽位置的命令: cluster keyslot key 如果set值，并在槽的范围内，返回OK 如果不在槽返回为返回moved异常 + 槽位置 + 节点位置。 1234567891011121314# 使用redis-cli设置值127.0.0.1:7000&gt; set hello worldOK# 因为php不在7000负责的槽范围内127.0.0.1:7000&gt; set php one(error) MOVED 9244 192.168.211.136:7001# 使用redis-cli-c集群模式连接127.0.0.1:7000&gt; set php best-&gt; Redirected to slot [9244] located at 192.168.211.136:7001OK192.168.211.136:7001&gt; get php"best" ask重定向当进行集群缩容和扩容时，由于槽处于迁移的过程。例如：一个slot存在三个key，分别为hello1、hello2、hello3，假设此时槽正在处于迁移状态，hello1已经迁移到了目标节点，此时如果在源节点获取hello1，则会报出ask重定向错误 move异常和ask异常 两者都是客户端重定向 moved: 槽已经确定迁移 ask： 槽还在迁移的过程中]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redisCluster]]></title>
    <url>%2F2019%2F04%2F11%2FredisCluster%2F</url>
    <content type="text"><![CDATA[sentinel问题引入上一篇章中，我们讲述了redis sentinel实现高可用。我们发现使用哨兵，每个slave都是全量存储数据，每个redis存储的内容都是完整的数据，浪费内存且有木桶效应。我们希望最大化的利用内存，采用集群，分布式存储。即每台redis都存储不同的内容。redisCluster是redis3.0推出的功能，有效的解决了redis分布式方面的需求，当遇到单机内存、并发、流量等瓶颈时，可采用cluster架构达到均衡负载的目的。 sentinel和clustersentinel与cluster是两个独立的功能，从特性来看sentinel可视为集群的子集，当不需要数据分片或已在客户端分片的场景下，sentinel足够使用了。如果需要进行水平扩容，那么cluster是一个很好的选择 数据分布集群首要解决把整个数据集按照分区规则映射到多个节点的问题，即把数据集划分到多个节点上，每个节点负责整个数据的一个子集 顺序分区 | 哈希分区 顺序分区保证每个节点的数据均衡 哈希分区 对每一个数字进行哈希函数，按照节点取余(实现之一) 对比 哈希分区探究redis-cluster采用哈希分区的方式进行数据分布，为此我们将进一步分析 节点取余分区对每一个key对3（节点个数）取余，如果余数为0则分布到第一个节点，余数为2则分布到第二个节点，余数为3则分布到第三个节点 客户端分片: 哈希 + 取余 节点伸缩： 节点变化，数据迁移量巨大 一致性哈希分区解决了节点取余的缺点token表示一个数据范围，为每一个节点分配一个token，每一个节点负责一部分token数据。当对key进行hash计算后按照顺时针的规则寻找离它最近的节点 客户端分片: 哈希 + 取余(优化取余) 节点伸缩： 只影响临近节点，但还是有数据迁移 虚拟槽分区Redis Cluster中有一个16384长度的槽的概念, 每个槽映射一个数据子集, 按照一定的hash计算对16383取余, 如果它落在某个槽的范围内，那么就证明这个槽必须所管理的数据，它是由服务端管理接节点槽之间的关系 节点取余分区和一致性哈希分区当节点伸缩时，都会进行数据的迁移，而rediscluster并不会有这种情况，因为每个节点负责槽的范围都是固定的，每个节点与槽的分配都具备权力 redisCluster架构 服务端有多个节点，每个节点都负责读写，节点之间是彼此通信的。如果节点A覆盖0－5460、节点B覆盖5461－10922、节点C覆盖10923－16383。 如果存入一个值，按照redis cluster哈希槽的算法： CRC16(‘key’)384 = 6782。 那么就会把这个key 的存储分配到 B 上了。同样，当我连接(A,B,C)任何一个节点想获取’key’这个key时，也会这样的算法，然后内部跳转到B节点上获取数据 基本架构 节点: 每个节点负责读写 meet: 完成节点之间通信工作 A meet C; A meet B; B &gt; C 指派槽: 只有给节点指派了槽才可以进行读写 复制: 为了达到高可用，有主从复制概念 rediscluster特性：复制、高可用、分片 原生命令安装 环境 3主3从 &gt; 192.168.211.134:7000(主) &gt; 192.168.211.134:7001(主) &gt; 192.168.211.134:7002(主) &gt; 192.168.211.134:7003(从) &gt; 192.168.211.134:7004(从) &gt; 192.168.211.134:7005(从) redis-[7000-70005].conf1234567891011port 7000daemonize yesdir ./logfile "7000.log"dbfilename "dump-7000.rdb"# 开启clustercluster-enabled yes# cluster配置文件cluster-config-file nodes-7000.conf# 如果有一个master宕机，没有故障恢复，整个集群不可用（默认true）cluster-require-full-coverage no 其他配置文件只需修改端口即可 启动123456789101112131415161718[root@localhost cluster]# redis-server redis-7000.conf[root@localhost cluster]# redis-server redis-7001.conf[root@localhost cluster]# redis-server redis-7002.conf[root@localhost cluster]# redis-server redis-7003.conf[root@localhost cluster]# redis-server redis-7004.conf[root@localhost cluster]# redis-server redis-7005.conf[root@localhost cluster]# ps -ef | grep redisroot 57254 1 0 01:44 ? 00:00:00 redis-server *:7000 [cluster]root 57258 1 0 01:44 ? 00:00:00 redis-server *:7001 [cluster]root 57262 1 0 01:44 ? 00:00:00 redis-server *:7002 [cluster]root 57266 1 0 01:44 ? 00:00:00 redis-server *:7003 [cluster]root 57270 1 0 01:44 ? 00:00:00 redis-server *:7004 [cluster]root 57274 1 0 01:44 ? 00:00:00 redis-server *:7005 [cluster]# 测试连接[root@localhost cluster]# redis-cli -p 7000127.0.0.1:7000&gt; set hello world(error) CLUSTERDOWN The cluster is down 我们可发现当set值时，错误表示当前集群是一个下线状态，因为在集群模式下，只有成功分配了槽且16383都进行了完整的分配，才可进行对外提供服务 meet port meet port完成节点之间的通信 1234567891011121314151617[root@localhost cluster]# redis-cli -p 7000 cluster meet 192.168.211.134 7001OK[root@localhost cluster]# redis-cli -p 7000 cluster meet 192.168.211.134 7002OK[root@localhost cluster]# redis-cli -p 7000 cluster meet 192.168.211.134 7003OK[root@localhost cluster]# redis-cli -p 7000 cluster meet 192.168.211.134 7004OK[root@localhost cluster]# redis-cli -p 7000 cluster meet 192.168.211.134 7005OK[root@localhost cluster]# redis-cli -p 7000 cluster nodes9c736c0733323151ee5aeaa8444cf177ab9b312b 192.168.211.134:7005 master - 0 1554832945389 5 connected97ba85211fa2d68ebff5ae4848eb4a52a899be5a 192.168.211.134:7003 master - 0 1554832943331 4 connected0ed9079a0d32b7e926a96d6a03d431c1228deffb 192.168.211.134:7000 myself,master - 0 0 1 connected89d495be82b618256b5c79a56f6894658db4cfa8 192.168.211.134:7004 master - 0 1554832942315 3 connectedb4c274d2501d4d105c8b7acf7787a66287a7406a 192.168.211.134:7001 master - 0 1554832941298 0 connected8465de32a8e17d23134d8ae8d62f2c78c9e40ef2 192.168.211.134:7002 master - 0 1554832944358 2 connected 此时6个节点已经达成相互通信! 分配槽点redis总共有16384个槽点，并且只有主节点需要分配槽点，这里我们使用的是三主三从，因此将槽点均分为三等分:0–5460，5461–10922，10923–16383 首先我们编写一个sh脚本 123456789start=$1end=$2port=$3for slot in `seq $&#123;start&#125; $&#123;end&#125;`do echo "slot:$&#123;slot&#125;" redis-cli -h 192.168.211.134 -p $&#123;port&#125; cluster addslots $&#123;slot&#125;done 分别为7000、7001、7002三个从节点分配槽节点 123sh add_slots.sh 0 5460 7000sh add_slots.sh 5461 10922 7001sh add_slots.sh 10923 16383 7002 查看槽信息 设置值123redis-cli -c -p 7000127.0.0.1:7000&gt; set hello worldOK 主从关系分配 cluster replicate node-id完成 7000-&gt;7003、7001-&gt;7004、7002-&gt;7005 12345678910111213141516[root@localhost cluster]# redis-cli -p 7003 cluster replicate 0ed9079a0d32b7e926a96d6a03d431c1228deffbOK[root@localhost cluster]# redis-cli -p 7004 cluster replicate b4c274d2501d4d105c8b7acf7787a66287a7406aOK[root@localhost cluster]# redis-cli -p 7005 cluster replicate 8465de32a8e17d23134d8ae8d62f2c78c9e40ef2OK[root@localhost cluster]# redis-cli -p 7000 cluster nodes# 可以发现7005-7004都成为了salve9c736c0733323151ee5aeaa8444cf177ab9b312b 192.168.211.134:7005 slave 8465de32a8e17d23134d8ae8d62f2c78c9e40ef2 0 1554833875305 5 connected97ba85211fa2d68ebff5ae4848eb4a52a899be5a 192.168.211.134:7003 slave 9c736c0733323151ee5aeaa8444cf177ab9b312b 0 1554833874286 5 connected89d495be82b618256b5c79a56f6894658db4cfa8 192.168.211.134:7004 slave b4c274d2501d4d105c8b7acf7787a66287a7406a 0 1554833871215 3 connected# 3个主节点0ed9079a0d32b7e926a96d6a03d431c1228deffb 192.168.211.134:7000 myself,master - 0 0 1 connectedb4c274d2501d4d105c8b7acf7787a66287a7406a 192.168.211.134:7001 master - 0 1554833873263 0 connected8465de32a8e17d23134d8ae8d62f2c78c9e40ef2 192.168.211.134:7002 master - 0 1554833876331 2 connected Ruby搭建集群ruby是官方推荐的搭建集群方式，自动分配槽和主从复制 安装ruby环境12345678# ruby依赖yum install ruby rubygems -y# gem-redis安装 https://rubygems.org/gems/redis/versions/3.0.0gem install -l redis-3.0.0.gem # 此时redis3.0.0/src目录有 redis.trib.rb文件，将它拷贝到/usr/lcoal/bincp redis-trib.rb /usr/local/bin/redis-trib redis-trib用于管理集群的命令 配置6个节点并启动12345678port 7001daemonize yesdir ./logfile "7001.log"dbfilename "dump-7001.rdb"cluster-enabled yescluster-config-file nodes-7001.confcluster-require-full-coverage no 其他节点修改端口即可，分别启动： 使用redis-trib搭建集群1redis-trib create --replicas 1 192.168.211.136:7001 192.168.211.136:7002 192.168.211.136:7003 192.168.211.136:7004 192.168.211.136:7005 192.168.211.136:7006 启动状态如下 cluster info打印集群信息 cluster nodes列出集群已知节点]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis集群——哨兵]]></title>
    <url>%2F2019%2F04%2F08%2Fredis%E9%9B%86%E7%BE%A4%E2%80%94%E2%80%94%E5%93%A8%E5%85%B5%2F</url>
    <content type="text"><![CDATA[主从复制问题引入当master和slave节点出现故障时，我们需要如何处理呢？首先我们知道故障是不可避免的。我们希望可做到高可用的故障转移，也就是说如果有一个服务宕机了，希望有另一台服务可顶替。通俗的将就是将故障进行转移，保证redis整体服务是可运行的 master故障如果slave出现故障，问题并不是很大，因为可以从master进行读写操作。如果master故障了，那么其他从节点将会断开与master的连接，此时客户端只可进行读的操作 master宕机处理首先我们要保证有写数据，可以将一台slave成为master，另外一台成为从节点 上述问题并没有解决自动故障转移的主要问题：自动让slave成为master，让其他slave同步master redis sentinel架构 Redis-Sentinel是Redis官方推荐的高可用性(HA)解决方案, Redis-sentinel本身也是一个独立运行的进程，它能监控多个master-slave集群，发现master宕机后能进行自动切换。它的主要功能有如下几点 不时地监控redis是否按照预期良好地运行 如果发现某个redis节点运行出现状况，能够通知另外一个进程(例如它的客户端) 能够进行自动切换。当一个master节点不可用时，能够选举出master的多个slave(如果有超过一个slave的话)中的一个来作为新的master,其它的slave节点会将它所追随的master的地址改为被提升为master的slave的新地址 sentinel核心配置12345678910111213# 去掉注释查看文件[root@localhost config]# cat sentinel.conf | grep -v "#" | grep -v "^$"# sentinel默认端口port 26379dir /tmp# 监控主节点 2标识你需要几个sentinel对master进行发现问题sentinel monitor mymaster 127.0.0.1 6379 2# 故障时间sentinel down-after-milliseconds mymaster 30000# 复制同时并发执行sentinel parallel-syncs mymaster 1# 故障转移时间sentinel failover-timeout mymaster 180000 sentinel实验实现如下配置 master-7000、slave-7001、slave-7002 sentinel-26379(默认端口)、sentinel-26380、sentinel-26381 开启主从节点redis-7000.conf(master) vim redis-{port}.conf: 分别添加如下配置12345port 7000daemonize yespidfile /var/run/redis-7000.pidlogfile "7000.log"dir ./ redis-7001.conf(slave)123456port 7001daemonize yespidfile /var/run/redis-7001.pidlogfile "7001.log"dir ./slaveof 192.168.211.134 7000 redis-7002.conf修改端口即可 启动一主二从123456789101112131415[root@localhost config]# redis-server redis-7000.conf [root@localhost config]# redis-server redis-7001.conf [root@localhost config]# redis-server redis-7002.conf [root@localhost config]# redis-cli -p 7000 pingPONG[root@localhost config]# redis-cli -p 7001 pingPONG[root@localhost config]# redis-cli -p 7002 pingPONG[root@localhost config]# redis-cli -p 7000 info replication# Replicationrole:masterconnected_slaves:2slave0:ip=192.168.211.134,port=7001,state=online,offset=197,lag=0slave1:ip=192.168.211.134,port=7002,state=online,offset=197,lag=1 开启sentinelsentinel-26379.conf12345678port 26379daemonize yesdir /tmplogfile "26379.log"sentinel monitor mymaster 192.168.211.134 7000 2sentinel down-after-milliseconds mymaster 30000sentinel parallel-syncs mymaster 1sentinel failover-timeout mymaster 180000 其他sentinel-26380.conf、sentinel-26381.conf修改端口即可 测试sentinel启动sentinel123[root@localhost config]# redis-sentinel redis-sentinel-26379.conf [root@localhost config]# redis-sentinel redis-sentinel-26380.conf [root@localhost config]# redis-sentinel redis-sentinel-26381.conf sentinel info12345678910111213[root@localhost config]# redis-cli -p 26379127.0.0.1:26379&gt; set hello world(error) ERR unknown command 'set'127.0.0.1:26379&gt; pingPONG127.0.0.1:26379&gt; info# Sentinel 的一些信息sentinel_masters:1sentinel_tilt:0sentinel_running_scripts:0sentinel_scripts_queue_length:0# sentinels=3 有三个sentinelmaster0:name=mymaster,status=ok,address=192.168.211.134:7000,slaves=2,sentinels=3 我们可发现sentinel监控的master和开启了多少个sentinel进行监控主从的信息 查看sentinel的变化1234567891011121314[root@localhost config]# cat redis-sentinel-26379.conf port 26379daemonize yesdir "/tmp"logfile "26379.log"sentinel monitor mymaster 192.168.211.134 7000 2sentinel config-epoch mymaster 0sentinel leader-epoch mymaster 0# 【关键的信息】：sentinel发现了master有两个从节点sentinel known-slave mymaster 192.168.211.134 7001# Generated by CONFIG REWRITE# 【关键的信息】：sentinel发现了master有两个从节点sentinel known-slave mymaster 192.168.211.134 7002sentinel current-epoch 0 sentinel会自动发现slave信息 客户端连接sentinel为什么我们不直接连接master? 因为我们采用高可用的方式，如果服务器端master宕机了，sentinel会完成自动的故障转移，那么此时客户端就不知道master节点的ip了。 基本原理： 故障转移实验我们将实现从master-7000端口中不断的set值，然后将master进程kill看看sentinel会不会自动处理故障转移 mavean依赖12345678910111213141516171819202122232425&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.25&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;version&gt;1.1.9&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; ResisSentinelTest.java12345678910111213141516171819202122232425262728293031323334353637public class ResisSentinelTest &#123; private static Logger logger = LoggerFactory.getLogger(RedisPipelineTest.class); public static void main(String[] args) &#123; String masterName = "mymaster"; Set&lt;String&gt; sentinels = new HashSet&lt;&gt;(); sentinels.add("192.168.211.134:26379"); sentinels.add("192.168.211.134:26380"); sentinels.add("192.168.211.134:26381"); JedisSentinelPool sentinelPool = new JedisSentinelPool(masterName, sentinels); int count = 0; while (true) &#123; count++; Jedis jedis = null; try &#123; // 获取连接 jedis = sentinelPool.getResource(); // 不断的set值 int index = new Random().nextInt(10000); String key = "k-" + index; String value = "v-" + index; jedis.set(key, value); if (count % 100 == 0) &#123; logger.info("&#123;&#125; value is &#123;&#125; ", key, jedis.get(key)); &#125; TimeUnit.MICROSECONDS.sleep(100); &#125; catch (Exception e) &#123; logger.error(e.getMessage(), e); &#125; finally &#123; if (jedis != null) &#123; jedis.close(); &#125; &#125; &#125; &#125;&#125; 执行上述代码之后会不断的输出如下日志信息 当我们在进行kill master之后控制台会输出如下日志信息 当过了30秒之后sentinel会进行故障转移，恢复set的操作 我们可以查看salve-7001和slave和7002哪个成为了master1234[root@localhost config]# redis-cli -p 7002127.0.0.1:7002&gt; info replication# Replicationrole:master sentinel原理sentinel可以对redis节点作失败判定以及故障转移，在sentinel内部有三个定时任务作为基础来实现上述所描述的 每10秒每个sentinel对master和slave执行info 发现slave 确认主从关系 每两秒每个sentinel和master进行发布订阅模式的形式，达成整体的信息交互平台 通过sentinel:hello频道交互 交互对节点的”看法”和自身信息 每一秒每个sentinel对其他sentinel和redis执行ping 心跳检测、失败判定的依据 主观下线|客观下线sentinel核心配置1234sentinel monitor mymaster 127.0.0.1 6379 2 sentinel down-after-milliseconds mymaster 30000sentinel parallel-syncs mymaster 1sentinel failover-timeout mymaster 180000 monitor1sentinel monitor [master-group-name] [ip] [port] [quorum] 这一行用于告诉Redis监控一个master叫做mymaster，它的地址在127.0.0.1，端口为6379，票数是2，票数个数建议为奇数, 且个数=1/2+1 quorun：票数，sentinel需要协商同意master是否可到达的数量，举个例子，redis集群中有5个sentinel实例，其中master挂掉啦，如果这里的票数是2，表示有2个sentinel认为master挂掉啦，才能被认为是正真的挂掉啦 down-after-millisecondssentinel会向master发送心跳PING来确认master是否存活，如果master在“一定时间范围”内不回应PONG 或者是回复了一个错误消息，那么这个sentinel会主观地认为这个master已经不可用了。而这个down-after-milliseconds就是用来指定这个“一定时间范围”的，单位是毫秒。 主观下线：每个sentinel节点对redis节点的失败的”偏见” 客观下线：所有sentinel节点对redis节点失败”达成共识”（超过票数个统一） parallel-syncs在发生failover主从切换时，这个选项指定了最多可以有多少个slave同时对新的master进行同步，这个数字越小，完成主从故障转移所需的时间就越长，但是如果这个数字越大，就意味着越多的slave因为主从同步而不可用。可以通过将这个值设为1来保证每次只有一个slave处于不能处理命令请求的状态。 领导则选举我们知道之际上完成故障转移的任务只需要一个sentinel即可。选举通过sentinel is-master-down-by-addr命令都希望成为领导者，此命令的作用: 一确认下线判定，二是进行领导者选举 选举过程1）每个做主观下线的sentinel节点向其他sentinel节点发送上面那条命令，要求将它设置为领导者。 2）收到命令的sentinel节点如果还没有同意过其他的sentinel发送的命令（还未投过票），那么就会同意，否则拒绝。 3）如果该sentinel节点发现自己的票数已经过半且达到了quorum的值，就会成为领导者 4）如果这个过程出现多个sentinel成为领导者，则会等待一段时间重新选举 总结 redis-sentinel是redis实现高可用的方案：故障发现、故障自动转移、配置中心、客户端通知 redis-sentinel是redis2.8版本才开始正式使用 尽可能在不同的物理机上部署redis-sentinel所有节点 redis-sentinel中的sentinel节点个数应该为大于等于3且最好为奇数 客户端初始化连接的是sentinel节点集合，不再是具体的redis节点，但是sentinel只是配置中心不是代理 redis-sentinel通过三个定时任务实现了sentinel节点对主从节点、其余sentinel节点的监控 redis-sentinel在对节点做失败判定时分为主管下线和和客观下线]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis集群——复制]]></title>
    <url>%2F2019%2F04%2F07%2Fredis%E9%9B%86%E7%BE%A4%E2%80%94%E2%80%94%E5%A4%8D%E5%88%B6%2F</url>
    <content type="text"><![CDATA[主从复制 单机部署redis所存在的问题：机器故障数据转移、容量瓶颈、QPS瓶颈 主节点master负责写数据，从节点slave负责读数据，主节点定期把数据同步到从节点保证数据的一致性 主从复制的作用：数据副本（高可用分布式基础）、读写分离提高性能 两种实现方式命令slaveof和配置文件的方式 slaveof1234# 6380节点（从）复制6379节点（主）redis-6380 &gt; slaveof 192.168.211.134:6379 OK# 取消复制（不希望成为从），此时会断开连接（从数据并不会丢失，而是主的数据不会同步给从）redis-6380 &gt; slaveof no one OK 主从复制实验 我们通过配置文件的方式实现 6379作为主节点、6380为从节点 其中有两个中要的配置slaveof ip port、slave-read-only yes（期望从节点只作读操作）目的是达到主从复制数据的一致性效果。保证主节写操作，而从节点只作为读操作 cp redis.conf两份分别命名为 redis-6379.conf、redis-6380.conf redis-6379.conf12345678910111213141516# 后台启动daemonize yes #进程idpidfile /var/run/redis-6379.pid# 主节点密码masterauth &lt;master-password&gt;# 当前节点端口port 6379# 日志文件logfile "6379.log"# rdb文件dbfilename dump-6379.rdb# 设置主从关系slaveof &lt;masterip&gt; &lt;masterport&gt;# 只读，主节点不关心slave-read-only yes 开启服务并连接客户端1234567redis-server redis-6379.confredis-cli# 查看分片信息127.0.0.1:6379&gt; info replication role:master # 主节点connected_slaves:0 # 所连接的从节点个数 redis-6380.conf12345678daemonize yes # 开启守护者进程pidfile /var/run/redis-6380.pid #进程idport 6380logfile "6380.log"dbfilename dump-6380.rdb # rdb文件slaveof 192.168.211.134 6379 # 成为6379的从节点# 从节点只作为读操作slave-read-only yes 开启服务并连接客户端123456redis-server redis-6379.confredis-cli -p 6380127.0.0.1:6379&gt; info replication # 查看分片信息role:slave # 已成为从节点master_host:192.169.211.134 # 主节点地址master_port:6379 # 主节点端口 测试从主节点6379端口中set值，从节点会复制主节点的数据 日志分析主从复制的读写分离实际上内部使用命令的方式进行数据的同步，我们可以分析两个日志文件 6379.log 123456# 表示6380需要进行复制操作Slave 192.168.211.134:6380 asks for synchronization# resync 全量复制操作Full resync requested by slave 192.168.211.134:6380# 可以发现复制操作是使用RDB进行实现的，实际就是将快照进行同步Starting BGSAVE for SYNC with target: disk 6380.log 123456789101112131415# 连接主节点Connecting to MASTER 192.168.211.134:6379# 主从复制MASTER &lt;-&gt; SLAVE sync started# 拿到master的runid（每一个redis启动时的随机ID）Full resync from master: f45a09af5ab5fe887ad7a33b451e4955b1dafebd:1# 拿到master节点数据(rbd)receiving 31 bytes from master# 清空之前的数据(全量复制的情况下)Flushing old data# 加载rdbLoading DB in memory# 加载成功Finished with success 验证Flushing old data我们可以看到上述6380日志，当拿到master节点之后做的事情，清空旧的数据、加载RDB，如下实验验证是否安装该流程进行的 先将6380成为master 12redis-cli -p 6380&gt; slaveof no one 切换到6379添加数据 12redis-cli -p 6379mset a b c d 再切换到6380执行如下操作 1234567891011# 此时成为master当然不能同步数据keys *(empty list or set)# 设置一些数据127.0.0.1:6380&gt; set abc6380 hello127.0.0.1:6380&gt; slaveof 192.168.211.134 6379OK127.0.0.1:6380&gt; keys *1) "c"2) "a" 我们可以看到当slaveof继续成为从节点时候，再获取数据时之前的abc6379已被删除 全量复制我们在分析日志的时候发现，master有resync(全量复制)、slave有获取masterRUNID的一些操作，引入我们引入两个概念 runid 以及 偏移量 runid每一个redis服务启动时，都会随机一个字符串id，作为一个标识。如果重启之后，runid会消失 查看runid1234[root@localhost config]# redis-cli -p 6379 info server | grep runrun_id:f45a09af5ab5fe887ad7a33b451e4955b1dafebd[root@localhost config]# redis-cli -p 6380 info server | grep runrun_id:e93481fdcc9e791516c77f8a61a3e7e5e2db4f9e 当复制时发现和之前的 run_id不同时（重启），将会对数据全量同步，一般用于初次复制场景 偏移量一个数据写入量的字节(如 set a b)，此时从节点会同步记录偏移量，当主从偏移量达到一至时，那么就完成了数据同步的过程。简言之：通过对比主从节点的复制偏移量，可以判断主从节点数据是否一致 如果主、从偏移量不一至。master &gt; slave可能会出现主从不一致 查看偏移量12[root@localhost redis-3.0.7]# redis-cli -p 6379 info replicationmaster_repl_offset:48425 当我们添加值时，偏移量也会随着增加 查看主、从偏移量1234[root@localhost redis-3.0.7]# redis-cli -p 6379 info replicationmaster_repl_offset:49146[root@localhost redis-3.0.7]# redis-cli -p 6380 info replicationslave_repl_offset:49170 为什么主从偏移量不一致呢？实际上主从的偏移量是一个同步更新的状态，从节点会将一些状态向主节点上报。我们也可查看主从的同步状态：123[root@localhost redis-3.0.7]# redis-cli -p 6379 info replicationslave0:ip=192.168.211.134,port=6380,state=online,offset=49482,lag=2master_repl_offset:49482 通常offset差距不会太大，否则可能有其他问题如网络、阻塞 原理 如果对于一个存了很多数据的master，slave期望复制master中的数据，并且这些数据是时刻同步的、完整的。这样就可达到完整的数据同步效果。 首先将本身的RDB文件同步给slave，为了达到时刻同步，在此期间，master写入的命令也会记录下来。当slave将RDB加载完后，会通过偏移量的对比将这期间master写入的值同步给slave 首先slave发送命令psync ? -1，参数1:runid，参数2：偏移量（报告主节点偏移量）。由于第一次复制不知道runid和偏移量，所以参数为?和-1 master接收到此命令，将runid和偏移量发送给slave slave保存master的基本信息 master会执行bgsave（rdb生成） 此时怎么将RDB发送给salve呢？master内部有复制缓冲区reple_back_buffer，它可记录最新的数据 经过了缓冲区的过滤，将RDB和缓存的一些信息发送给slave(5-6步骤) slave先清空自己old数据 加载RDB以及缓存数据完成同步 部分复制经过上述分析，我们可以明显感受到，全量复制的开销是巨大的。有如下几个问题 bgsave时间，需要生成RDB RDB文件网络传输时间 slave清空old的时间 slave加载RDB的时间 可能的AOF重写时间 为什么需要部分复制 在redis2.8版本之前，如果master和slave之间的网络发生了抖动连接断开，就会导致slave完全不知道master的动作，同步就会出问题，而为了保证数据一致，等网络恢复后进行一次全量复制。而全量复制的开销是很大的，redis2.8版本就提个了一个部分复制的功能 部分复制的实现原理当master和slave断开连接时，master会将期间所做的操作记录到复制缓存区当中（可以看成是一个队列，其大小默认1M）。待slave重连后，slave会向master发送psync命令并传入offset和runId，这时候，如果master发现slave传输的偏移量的值，在缓存区队列范围中，就会将从offset开始到队列结束的数据传给slave，从而达到同步，降低了使用全量复制的开销]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elasticsearch+filebeat+kibana日志收集]]></title>
    <url>%2F2019%2F04%2F06%2FELK%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86%2F</url>
    <content type="text"><![CDATA[日志对大规模日志数据进行采集、追踪、分析及处理 目前主流的分布式日志框架有： Logstash ELK(ElasticsSearch, Logstash, Kibana) – 一套强大的日志收集系统，由于Logstash消耗资源过大，官方推荐使用filebeat取代 Flume – 由Apache基金会提供的一个分布式、高可靠、高可用的服务 filebeat用于日志的收集、elasticsearch用于存储日志、kibana实现数据的展示 download: https://www.elastic.co/cn/downloads/past-releases elasticsearch install es启动不能使用root，因此需要创建一个用户组 123groupadd esuseradd es -g espasswd es 切换root进入es目录为es文件给予权限 1chown -R es:es es es-5.6.16 修改/conf/elasticsearch.yml 1234567# Network # 绑定ipnetwork.host: ip # 默认http端口http.port: 9200# 默认tcp端口transport.tcp.port: 9300 修改jvm内存大小 /conf/jvm.options 12-Xms1g-Xms1g 由于es启动需要开启大量线程，需要修改系统文件 1234567891011121314# vi /etc/security/limits.conf* soft nofile 65536* hard nofile 131072* soft nproc 2048* hard nproc 4096* soft memlock unlimited* hard memlock unlimited# cd /etc/security/limits.dvi 20-nproc.confroot soft nproc 2048# vi /etc/sysctl.confvm.max_map_count=655360 su es &gt; ./elasticsearch 如果启动不了设置防火墙 filebeatFilebeat是一个轻量型日志采集器，在你的服务器上安装客户端后，filebeat会监控日志目录或者指定的日志文件，追踪读取这些文件（追踪文件的变化，不停的读），并且转发这些信息到elasticsearch或者logstarsh中存放 安装完之后编辑核心配置文件filebeat.yml 12345678910111213# 指定被探测的日志文件, 可添加多个该配置paths: - /install/logs/*.log# 配置多行合并multiline.pattern: ^\[multiline.negate: truemultiline.match: after # 指定es的位置，将日志收集到es存储output.elasticsearch: # Array of hosts to connect to. hosts: ["192.168.211.134:9200"] ./filebeat 启动 日志收集测试 : 当配置的探测日志文件发生变化时，filebeat会将数据发送给ES进行存储 kibanakibana可取代elastic-head，提供了更为专业的可视化界面、日志分析系统，它是ES的成员之一 安装完之后配置kibana.yml核心配置文件12345678# 默认端口server.port: 5601# 绑定ipserver.host: "192.168.211.134"# 指定es的地址elasticsearch.url: "http://192.168.211.134:9200"# 启动cd bin &gt; ./kibana]]></content>
      <categories>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
        <tag>Kibana</tag>
        <tag>Logstash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis的持久化]]></title>
    <url>%2F2019%2F04%2F06%2Fredis%E7%9A%84%E6%8C%81%E4%B9%85%E5%8C%96%2F</url>
    <content type="text"><![CDATA[持久化的作用redis的所有数据存储到内存中，如果redis-server进程退出，那么数据将丢失。为了解决这个问题 Redis 提供了两种持久化的方案，将内存中的数据保存到磁盘中，避免数据的丢失 RDB 该机制使用快照方式，在指定的时间内将内存中的数据写入到硬盘（rdb文件），也可手动执行命令 文件策略：如果存在老的RDB文件，会进行替换 优点：RBD作为一个备份文件容易恢复，性能好，通过子进程fork生成rdb，用于备份 缺点：容易造成数据丢失，如果fork花费时间大，那么将会阻塞redis服务、耗时耗性能 触发机制 save命令（同步 ） — 可能会产生阻塞 bgsave命令（异步）— 创建一个子进程fork() 生成rdb文件（阻塞发生在fork），创建完毕返回给主进程bgsave successfully 自动 — 根据save配置自动生成，例如如果在60秒内改变了1w条数据则生成rdb、300-10、900-1满足任一个条件（底层使用fork） 在redis.conf中节点为SNAPSHOTTING中有如下配置 12345678910111213save 900 1save 300 10save 60 10000# bgsave发生错误是否停止写入stop-writes-on-bgsave-error yes# rdb是否采用压缩格式rdbcompression yes# rbd校验rdbchecksum yes# 默认rbd文件名，在/bin目录dbfilename dump.rdb# 日志等文件存储位置dir ./ 推荐配置 12345678# 不采用自动生成# rbd文件名称采用+端口形式区分dbfilename dump-$&#123;port&#125;.rdb# 建议指定容量较大硬盘位置dir /bigdiskpath# 其他默认 不容忽略的方式：主从复制、debug reload、shutdown可能会触发 AOFRDB快照方式并不是很可靠，如果服务器宕机，那么最新的数据就会丢失。而AOF文件提供一种更为可靠的的持久化方式。当进行set操作时，会追加到AOF文件中。当redis重启之后，AOF中的命令会被重新执行一次，重建数据 AOF的三种策略 always：每条命令都会持久化一次（执行写操作时，先进入缓冲区后写入到硬盘） everysec：每秒持久化一次 no：缓冲区的刷新策略根据OS决定 AOF重写由于每条命令都会追加到AOF文件中，随着时间的推移，AOF文件必然逐渐变大。AOF重写解决了这一问题 例 ：命令优化、过期数据 123set hello oneset hello twoset hello three 当连续执行上诉命令最终的结果为three，而AOF并不会写入三次命令，而只会追加有效的命令set hello three AOF重写实现的两种方式 bgrewriteaof命令：（类似RDB的bgsave）开启子线程完成AOF重写 AOF重写配置 在/redis.conf配置文件有APPEND ONLY MODE节点 123456789101112# 要完成aof功能开启为yesappendonly no# 文件名appendfilename "appendonly.aof"# 三种同步策略# appendfsync alwaysappendfsync everysec -- 每秒写入（默认）# appendfsync no# 写入aof文件，不等待磁盘同步（安全，但可能阻塞）no-appendfsync-on-rewrite no RDB和AOF的抉择 比较 RDB最佳策略 “关闭”rdb 集中管理，指定时间内进行大量的数据备份 从节点开启 分片 AOF最佳策略 “开启” ： 缓存和存储 AOF重写集中管理 everysec策略 分片 问题fork当进行 RDB的bgsave操作和AOF的bgrewriteaof会产生子线程进行持久化的相关操作，如果fork操作执行慢，此时会将redis主线程阻塞。 查看fork持久化的时间 1latest_fork_usec 改善 物理机或支持fork操作的虚拟化技术 控制redis实例最大可用内存：maxmemory 合理配置Linux内存分配策略：vm.overcommit_memory=1（默认0） 放宽AOF重写自动触发机制，不必全量复制 进程外开销 bgsave和bgrewriteaof是将内存中的数据写入到硬盘，此时会集中消耗CPU、rork内存开销、硬盘 优化 不做CPU绑定，不和CPU密集部署，单机部署不要大量的AOF和RDB持久化的过程 echo never &gt; 追加配置文件 不要和高硬盘负载服务部署在一起：存储服务、消息队列等 配置no-appendfsync-on-rewrite yes ssd介质 AOF追加阻塞如果我们使用了AOF的每秒刷盘策略，如果fork执行大于2秒，那么主线程阻塞 阻塞定位 redis日志 info persistence收集记录]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[瑞士军刀redis]]></title>
    <url>%2F2019%2F04%2F05%2F%E7%91%9E%E5%A3%AB%E5%86%9B%E5%88%80redis%2F</url>
    <content type="text"><![CDATA[慢查询许多存储系统（例如MySQL）提供慢查询日志帮助开发和运维人员定位系统存在的慢操作。所谓慢查询日志就是系统在命令执行前后计算每条命令的执行时间，当超过预设阈值，就将这条命令的相关信息（例如：发生时间、耗时、命令的详细信息）记录下来，Redis也提供了类似的功能。 首先我们需要先了解client和serve的生命周期 慢查询发生在第三阶段 客户端超时不一定慢查询，但慢查询是客户端超时的一个可能因素 Pipeline多数情况下，我们会通过请求-响应的机制来操作redis。当要执行多个命令时，由于redis是单线程的，那么下一次请求必须等待上一次请求完成之后才可继续执行。而pipeline模式，客户端可一次性将命令打包发送， 无需等待服务端返回。这样就减少了网络往返时间。 使用传统方式 123456789@Testpublic void test1() &#123; long start = System.currentTimeMillis(); for (int i = 0; i &lt; 10000; i++) &#123; jedis.hset("hashKey:" + i, "field" + i, "value" + i); // hashkey1 -&gt; field:1 : value:1 &#125; long end = System.currentTimeMillis(); System.out.println("花费时间为: " + (end - start)); //2983&#125; 使用pipeline 12345678910@Testpublic void test2() &#123; Pipeline pipeline = jedis.pipelined(); long start = System.currentTimeMillis(); for (int i = 0; i &lt; 10000; i++) &#123; pipeline.hset("hashKey: " + i, "field" + i, "value" + i); // hashkey1 -&gt; field:1 : value:1 &#125; long end = System.currentTimeMillis(); System.out.println("花费时间为: " + (end - start)); // 38&#125; bitmap位图：基于最小的单位bit进行存储，非常省空间、二进制数据存储，计算快 12# 取出位图指定索引的值getbit element 0 使用场景：独立用户统计（一亿用户，五千万独立） set存储: 内存量200mb bitmap存储：内存量12.5mb]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis api]]></title>
    <url>%2F2019%2F04%2F05%2Fredis-api%2F</url>
    <content type="text"><![CDATA[通用命令 keys[patten]（查看所有的key） dbsiz（查看所有key的大小） exists key（判断一个key是否存在） del key [key ..] （删除key） expice key seconds（为key设置过期时间） type key（查看key类型） 内部编码架构 单线程架构 当我们在操作redis时，时刻要明白redis在一个瞬间只会执行一条命令，不会执行两条命令 单线程为何快？ 纯内存 非阻塞IO 避免线程之间的切换和竞态消耗 string redis所有的key都为字符串，value可存储普通字符串、数值、二进制 使用场景：缓存、计数器、分布式锁 字符串的value不能大于512MB 基本命令 get、set、del（O1）： 12345678# 获取key对应的valueget key # 设置key-valueset key value# 删除key-valuedel key 计数命令 incr 、decr、incrby、decrby（O1）： 123456789101112# key自增1，0开始incr key# 相反decr key# 期望自增指定值，key自增kincrby key k# 相反decrby key k# 浮点数的自增3.5incrbyfloat key 3.5 其他set命令 set、setnx、setxx 12345678# 不管key是否存在，都设置set key value# key不存在才进行设置（add操作）setnx key value# key存在才设置（update操作）set key value xx 批量处理命令：mget mset 12345# 批量获取keymget key1 key2 ..# 批量设置key-valuemset key1 value1 key2 value2 ... 如果传输n次get命令，那么 n次get = n次网络时间 + n次命令时间 如果1次将命令批量传输给服务端，那么 1次mget = 1次网络时间 + n次命令时间 其他命令 12345678# 为key设置新的值，并返回之前的值getset key newvalue# 为key追加值append key value# 返回字符串长度strlen key 使用场景： 记录网站每个用户个人主页的访问量 1incr userid:pageview 缓存视频的基本信息（数据源在mysql） 1231. 从redis中取数据，存在数据直接返回，则不需要访问mysql2. 若不存在，访问mysql，获取到数据，加入到redis缓存3. 当下此访问此接口时，会从redis中访问 hashhash键值结构： 基本命令 12345678# 获取hash key对应的field的valuehget key field# 设置hash key对应file的valuehset key field value# 删除hash key对应file的valuehdel key field value hexists、hlen 12345# 判断hash key是否有filedhexists key filed# 获取hash key fiedl的数量hlen key 批量操作 12345# 批量获取hash key的一批field对应的值hmget key field1 field2...# 批量设置hash key的一批field valuehmset key field1 value2 ... 其他命令 12345678# 返回hash key对应的所有field和valuehgetall key# 返回所有的值hvals key# 返回所有的fieldhkeys key 使用场景 和string 的使用场景类似、数据缓存等 listlist 队列结构：有序、可重复、左右两边插入弹出 增加操作 12345678# 从列表左边插入值lpush key val1 val2# 从列表右边插入值rpush key val1 val2 # 在列表指定值的前|后插入新的值linsert key before|after value newvalue 删除操作 123456789101112# 从列表左边弹出一个元素lpop key# 从列表右边弹出一个元素rpop key# 从列表删除所有value相等的值# count=0删除所有、count&gt;0左边开始删除、count&lt;0右边开始删除lrem key count value# 保留指定索引内的列表元素ltrim key start end 查询操作 12345678# 获取索引内范围的元素lrange key start end(包含)# 根据指定索引获取元素lindex key index# 列表长度llen key 修改操作 12# 设置指定索引的值lset key index newvalue 使用场景 用户抢购进入排队状态（加入队列） 你关注的人更新文博（LPUSH可看到最新状态、lrange范围查询） set集合特点：无序、无重复、可进行集合间操作（交集、并集） 基本命令 1234567891011121314151617181920# 向集合key添加元素（如果元素存在添加失败）sadd key element# 删除集合中某个元素srem key element# 计算集合sizescard key# 判断元素是否在在集合中存在sismember key element# 从集合中挑选一个元素（不删除）srandmember key# 从集合中弹出一个元素(删除)spop key # 取出集合中所有元素smembers key]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis初识]]></title>
    <url>%2F2019%2F04%2F05%2Fredis%E5%88%9D%E8%AF%86%2F</url>
    <content type="text"><![CDATA[引入redis是一款开源的nosql,基于key-value的键值对存储服务系统，支持多种数据结构。 redis的特性 多种数据结构 其中还扩展数据结构有 bitmaps：位图 ，（布隆过滤器可使用位图来实现） hyperloglog：超小内存唯一值计数（12k geo：地理信息位置 速度快 redis可达到10w读写速度，将数据存储到内存中，使用C语言编写（50000 line），单线程 持久化 功能丰富 主从复制 高可用、分布式 redis使用场景 缓存系统 先从cache中获取数据，如果有返回数据，没有则从数据库中取数据并加入到缓存中 计数器 消息队列系统 排行榜 社交网络 实时系统 redis的安装 http://download.redis.io/releases/ 1234567891011# 依赖yum install gcc-c++# 下载wget http://download.redis.io/releases/redis-3.0.7.tar.gz# cd到redis目录编译，会产生redis.conf、src等文件make# 安装到指定目录产生bin文件make install PREFIX=/install/redis# 如果客户端连接不了请关闭防火墙systemctl stop firewalld bin目录下6个可执行文件 redis-server：redis服务器 redis-cli：redis命令行客户端 redis-benchmark：基准测试 redis-check-aof：持久化aof文件修复工具 redis-check-dump：持久化rdb文件修复工具 redis-sentinel：sentinel服务器（2.8version） 三种启动redis方式 redis-serve（默认配置） redis-serve – port 6380（动态参数启动，默认端口是6379） redis-serve &amp;confipath（参与配置文件启动）]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F04%2F04%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
